{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c454e4c4-e71a-47b4-b5e9-b4cb526bba1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LakeflowAPIHandler"
    }
   },
   "outputs": [],
   "source": [
    "%run ./GatewayManagementHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918d2ab1-13a1-468c-8f49-47d5fbc5bcbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./IngestionManagementHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79fa56c1-8d02-40b9-9074-bff6c0d06497",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Modules"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import time\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "from pyspark.sql.functions import trim, col\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4434731-a121-4639-a108-d3977f12a192",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "IngestionOrchestrator"
    }
   },
   "outputs": [],
   "source": [
    "class IngestionOrchestrator(SqlIngestionCommon):\n",
    "    def __init__(self, p_environment,  p_internal_product_id, p_source_server_name, p_source_database_name=None, p_internal_client_id=None, p_internal_facility_id=None):\n",
    "        super().__init__()\n",
    "        self.params_dict = {\n",
    "            'p_environment': p_environment,\n",
    "            'p_internal_product_id': p_internal_product_id,\n",
    "            'p_source_server_name': p_source_server_name,\n",
    "            'p_source_database_name': p_source_database_name,\n",
    "            'p_internal_client_id': p_internal_client_id,\n",
    "            'p_internal_facility_id': p_internal_facility_id\n",
    "        }\n",
    "\n",
    "        self.process_max_workers = 10\n",
    "        self.trackeback_length = 1000\n",
    "\n",
    "    def set_process_max_workers(self, process_max_workers):\n",
    "        self.process_max_workers = process_max_workers\n",
    "\n",
    "    def get_process_max_workers(self):\n",
    "        return self.process_max_workers\n",
    "    \n",
    "    def set_traceback_length(self, traceback_length):\n",
    "        self.trackeback_length = traceback_length\n",
    "\n",
    "    def get_traceback_length(self):\n",
    "        return self.trackeback_length\n",
    "\n",
    "    def process(self):\n",
    "        v_return_dict = {'Total': 0, 'Success': 0, 'SuccessResults': [], 'ExecutionFailed' : 0, 'ExecutionFailures': []}\n",
    "\n",
    "        # Retrieve List of Pipelines to Run for Ingestion\n",
    "        process_config = ProcessConfigData(self.params_dict[\"p_environment\"])\n",
    "        df_config = process_config.get_table_list_aggregate (\n",
    "                                                                self.params_dict[\"p_internal_product_id\"], \n",
    "                                                                self.params_dict[\"p_source_server_name\"], \n",
    "                                                                self.params_dict[\"p_source_database_name\"], \n",
    "                                                                self.params_dict[\"p_internal_client_id\"], \n",
    "                                                                self.params_dict[\"p_internal_facility_id\"]\n",
    "                                                            )\n",
    "\n",
    "        # Separate New and Existing Pipelines\n",
    "        df_new = df_config.filter((col(\"IngestionPipelineId\").isNull()) | (trim(col(\"IngestionPipelineId\")) == \"\"))\n",
    "        df_existing = df_config.filter((col(\"IngestionPipelineId\").isNotNull()) & (trim(col(\"IngestionPipelineId\")) != \"\"))\n",
    "\n",
    "        # Process New Pipelines - Not Implemented\n",
    "        if df_new.count() > 0:\n",
    "            print(f\"New Pipelines to be implemented: {df_new.select('IngestionPipelineName').collect()}\")\n",
    "        else:\n",
    "            print(\"No new Pipelines to process\")\n",
    "\n",
    "        # Process Existing Pipelines\n",
    "        if df_existing.count() > 0:\n",
    "            print(f\"Existing Pipelines to process: {df_existing.select('IngestionPipelineName').collect()}\")\n",
    "        else:\n",
    "            print(\"No existing Pipelines to process\")\n",
    "\n",
    "        v_thread_errors = []       # To collect all exceptions\n",
    "        v_thread_results = []      # To collect successful results\n",
    "        v_row_array = []\n",
    "\n",
    "        # Get Unitity Catalog and Managed Location Root Path from Process Config Attributes\n",
    "        v_unity_catalog = process_config.get_config_attribute_value('AnalyticsUnityCatalog')\n",
    "        v_managed_location_root_path = process_config.get_config_attribute_value('AdlsAnalyticsFullpathUri')\n",
    "\n",
    "        # Construct Row Array for Processing Existing Pipelines and adding common Attributes\n",
    "        for row in df_existing.collect():\n",
    "            v_row_dict = row.asDict()\n",
    "            v_row_dict['Status'] = 'Pending'\n",
    "            v_row_dict['DestinationCatalog'] = v_unity_catalog\n",
    "            v_row_dict['ManagedLocationRootPath'] = v_managed_location_root_path\n",
    "            v_row_dict['process_config'] = process_config\n",
    "            v_row_dict['ProcessIdentifier'] = f\"Pipeline={v_row_dict['GatewayPipelineName']} Server={v_row_dict['SourceServerName1']} Database={v_row_dict['SourceDatabaseName1']}\"\n",
    "            v_row_array.append(v_row_dict)\n",
    "\n",
    "        # Launch Threads for Process Existing Pipelines in parallell\n",
    "        with ThreadPoolExecutor(max_workers=self.get_process_max_workers()) as executor:\n",
    "            futures = {executor.submit(self.process_row, obj): obj for obj in v_row_array}\n",
    "            for future in as_completed(futures):\n",
    "                obj = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    v_pipeline_id = result['pipeline_id'] if 'pipeline_id' in result else None\n",
    "                    v_thread_results.append(str(obj['ProcessIdentifier']) + ': Message=Ingestion Successfully Completed - : Summary' + str(result))\n",
    "                    #print(f\"Thread Successes: \", v_thread_results)\n",
    "                except Exception as e:\n",
    "                    v_thread_errors.append(str(obj['ProcessIdentifier']) + ': Message=' + str(e).replace('\\n','')[0:self.get_traceback_length()] + ' Summary:' + ' Traceback:' + traceback.format_exc().replace('\\n','')[0:self.get_traceback_length()] )\n",
    "                    #print(f\"Thread Errors: \", v_thread_errors)\n",
    "        # Collect Statistics for Summary\n",
    "        v_return_dict['Total'] = len(v_row_array)\n",
    "        v_return_dict['Success'] = len(v_thread_results)\n",
    "        v_return_dict['SuccessResults'] = v_thread_results\n",
    "        v_return_dict['ExecutionFailed'] = len(v_thread_errors)\n",
    "        v_return_dict['ExecutionFailures'] = v_thread_errors\n",
    "\n",
    "        print(\"\\n\\nSummary:\\n\")\n",
    "        pprint.pprint(v_return_dict, indent=4, compact=True, sort_dicts=False, width=10000)\n",
    "        # Raise Exception if error or nothing to process\n",
    "        if len(v_thread_errors) > 0:\n",
    "            raise Exception(f\"{len(v_thread_errors)} Execution errors were encountered during processing. See exception(s) for details.\")\n",
    "        elif len(v_row_array) == 0:\n",
    "            raise Exception(f\"Unable to find valid configuration(s) for Parameters: {self.params_dict}\")\n",
    "        else:\n",
    "            print(f\"Processing Complete. {len(v_thread_results)} of {len(v_row_array)} pipelines were processed successfully.\\n\\n\")\n",
    "        \n",
    "        return v_return_dict\n",
    "    \n",
    "    def process_row(self, p_row_dict):\n",
    "\n",
    "        v_return_dict = {\"started\": {}, \"edited\": {}, \"full_refresh\": {}, \"refresh\": {}, \"status\": {}}\n",
    "\n",
    "        # Get API Object\n",
    "        api_obj = LakeflowAPI()\n",
    "\n",
    "        # Start Related Gateway\n",
    "        v_gateway_state = self.start_gateway_pipeline(p_row_dict, api_obj)\n",
    "        \n",
    "        # Delay before Starting Pipeline\n",
    "        self.delay_pipeline_start(p_row_dict, api_obj)\n",
    "\n",
    "        # Start Ingestion pipeline\n",
    "        v_return_start_dict = self.start_ingestion_pipeline(p_row_dict, api_obj)\n",
    "        v_return_dict.update(v_return_start_dict)\n",
    "\n",
    "        # Stop Related Gateway\n",
    "        v_state_dict = self.stop_gateway_pipeline(p_row_dict, api_obj)\n",
    "\n",
    "        if 'IngestionState' in v_state_dict and v_state_dict['IngestionState'] == 'FAILED':\n",
    "            raise Exception (f\"Error: Ingestion Pipeline {p_row_dict['IngestionPipelineName']} failed - State: {v_state_dict['IngestionState']} - {v_return_dict} \")\n",
    "        \n",
    "        return v_return_dict   \n",
    "\n",
    "    def start_ingestion_pipeline(self, p_row_dict, api_obj=None):\n",
    "        v_return_dict = {\"started\": {}, \"edited\": {}, \"full_refresh\": {}, \"refresh\": {}, \"status\": {}}\n",
    "        v_config_table_array = []\n",
    "        v_pipeline_table_array = []\n",
    "        v_config_table_full_refresh_array = []\n",
    "\n",
    "        print(f\"Initialize Ingestion Pipeline {p_row_dict['IngestionPipelineName']}....\")\n",
    "\n",
    "        # Set Process Status to Pending\n",
    "        self.set_pipeline_process_status(p_row_dict, 'P')\n",
    "\n",
    "        # Get Pipeline Specs for Comparison\n",
    "        v_pipeline_dict = api_obj.get_pipeline(p_row_dict['IngestionPipelineId'])\n",
    "\n",
    "        if v_pipeline_dict['status'] == 'ok':\n",
    "\n",
    "            # Compare Config Tables vs Pipeline Tables\n",
    "            v_config_table_array = self.build_config_source_table_array(p_row_dict)\n",
    "            v_config_table_full_refresh_array = self.build_config_source_table_array(p_row_dict, 1)\n",
    "            v_pipeline_table_array = v_pipeline_dict['response']['spec']['ingestion_definition']['objects']\n",
    "            v_diff_dict = self.compare_config_vs_pipeline_tables(v_config_table_array, v_pipeline_table_array, v_config_table_full_refresh_array)\n",
    "\n",
    "            # Save Snapshot Start Time\n",
    "            v_snapshot_start_time = self.get_snapshot_startime()\n",
    "\n",
    "            # Based on Compare Config Results - Start/Edit Pipeline, Perform Incremental or Full Refresh\n",
    "            if v_diff_dict[\"added_count\"] == 0 and v_diff_dict[\"removed_count\"] == 0 and v_diff_dict[\"full_refresh_count\"] == 0:\n",
    "                # Start Pipeline Refresh\n",
    "                print(f\"Pipeline={p_row_dict['IngestionPipelineName']} - Incremental Refresh of {v_diff_dict['config_count']} Normal Run/No Changed Tables\")\n",
    "                v_return_dict[\"started\"] = api_obj.start_pipeline(p_row_dict['IngestionPipelineId'])\n",
    "                v_return_dict[\"status\"][\"started\"] = v_return_dict[\"started\"][\"status\"]\n",
    "                self.wait_for_ingestion_pipeline_idle(p_row_dict, api_obj)\n",
    "            elif v_diff_dict[\"added_count\"] == 0 and v_diff_dict[\"removed_count\"] == 0 and v_diff_dict[\"full_refresh_count\"] ==  v_diff_dict[\"config_count\"]:\n",
    "                # Full Refresh - All Tables\n",
    "                print(f\"Pipeline={p_row_dict['IngestionPipelineName']} - Full Refresh of ALL {v_diff_dict['config_count']} Tables\")\n",
    "                v_return_dict[\"full_refresh\"] = api_obj.start_pipeline(p_row_dict['IngestionPipelineId'], True)\n",
    "                v_return_dict[\"status\"][\"full_refresh\"] = v_return_dict[\"full_refresh\"][\"status\"]\n",
    "                self.wait_for_snapshots_complete(p_row_dict, v_diff_dict['config_count'], v_snapshot_start_time)\n",
    "                v_return_dict[\"full_refresh\"] = api_obj.start_pipeline(p_row_dict['IngestionPipelineId'])\n",
    "                self.wait_for_ingestion_pipeline_idle(p_row_dict, api_obj)\n",
    "            else:\n",
    "                if v_diff_dict[\"full_refresh_count\"] > 0:\n",
    "                    # Full Refresh List of Tables\n",
    "                    print(f\"Pipeline={p_row_dict['IngestionPipelineName']} - Full Refresh of {v_diff_dict['full_refresh_count']} Table(s)\")\n",
    "                    v_table_name_list = [ tbl['table']['source_table'] for tbl in v_diff_dict[\"full_refresh\"]]\n",
    "                    v_return_dict[\"full_refresh\"] = api_obj.start_pipeline(p_row_dict['IngestionPipelineId'], False, None, v_table_name_list)\n",
    "                    v_return_dict[\"status\"][\"full_refresh\"] = v_return_dict[\"full_refresh\"][\"status\"]\n",
    "                    self.wait_for_snapshots_complete(p_row_dict, v_diff_dict[\"full_refresh_count\"], v_snapshot_start_time)\n",
    "                    v_return_dict[\"full_refresh\"] = api_obj.start_pipeline(p_row_dict['IngestionPipelineId'])\n",
    "                    self.wait_for_ingestion_pipeline_idle(p_row_dict, api_obj)\n",
    "\n",
    "                if v_diff_dict[\"refresh_count\"] > 0:\n",
    "                    # Incremental Refresh List of Tables\n",
    "                    print(f\"Pipeline={p_row_dict['IngestionPipelineName']} - Incremental Refresh of {v_diff_dict['refresh_count']} Table(s)\")\n",
    "                    v_table_name_list = [ tbl['table']['source_table'] for tbl in v_diff_dict[\"refresh\"]]\n",
    "                    v_return_dict[\"refresh\"] = api_obj.start_pipeline(p_row_dict['IngestionPipelineId'], False, v_table_name_list)\n",
    "                    v_return_dict[\"status\"][\"refresh\"] = v_return_dict[\"refresh\"][\"status\"]\n",
    "                    self.wait_for_ingestion_pipeline_idle(p_row_dict, api_obj)\n",
    "                    \n",
    "                if (v_diff_dict[\"added_count\"] + v_diff_dict[\"removed_count\"]) > 0:\n",
    "                    # Edit Pipeline - Add/Remove Tables\n",
    "                    print(f\"Pipeline={p_row_dict['IngestionPipelineName']} - Edit Pipeline - Add/Remove {v_diff_dict['added_count']}/{v_diff_dict[\"removed_count\"]} Table(s)\")\n",
    "                    v_edit_json_dict = v_pipeline_dict['response']['spec']\n",
    "                    v_edit_json_dict['ingestion_definition']['objects'] = v_config_table_array\n",
    "                    v_return_dict[\"edited\"] = api_obj.update_pipeline(p_row_dict['IngestionPipelineId'], v_edit_json_dict)\n",
    "                    v_return_dict[\"status\"][\"edited\"] = v_return_dict[\"edited\"][\"status\"]\n",
    "                    self.wait_for_ingestion_pipeline_idle(p_row_dict, api_obj)\n",
    "                    if v_diff_dict[\"added_count\"] > 0:\n",
    "                        # Trigger Pipeline Full Refresh for Added Tables\n",
    "                        v_table_name_list = [ tbl['table']['source_table'] for tbl in v_diff_dict[\"added_count\"]]\n",
    "                        v_return_dict[\"added\"] = api_obj.start_pipeline(p_row_dict['IngestionPipelineId'], False, None, v_table_name_list)\n",
    "                        v_return_dict[\"status\"][\"added\"] = v_return_dict[\"added\"][\"status\"]\n",
    "                        self.wait_for_ingestion_pipeline_idle(p_row_dict, api_obj)\n",
    "\n",
    "        print(f\"Start Ingestion Pipeline {p_row_dict['IngestionPipelineName']}....\")\n",
    "\n",
    "        return v_return_dict\n",
    "\n",
    "    def compare_config_vs_pipeline_tables(self, p_config_array, p_pipeline_array, p_config_full_refresh_array=[]):\n",
    "        # Initialize Return Dictionary with Counts and Lists\n",
    "        v_return_dict = {\"added\": [], \"removed\": [], 'unchanged': [], \"full_refresh\": [], \"refresh\": [], \n",
    "                        \"added_count\": 0, \"removed_count\": 0, \"unchanged_count\": 0, \"config_count\": 0, \"pipeline_count\": 0, \"full_refresh_count\": 0, \"refresh_count\": 0 }\n",
    "        \n",
    "        v_pipeline_dict = {}\n",
    "        v_config_dict = {}\n",
    "        v_config_full_refresh_dict = {}\n",
    "\n",
    "        # Construct unique Ids for comparison - List of Tables Retrieved from Config\n",
    "        for v_config in p_config_array:\n",
    "            v_id = v_config[\"table\"][\"source_catalog\"] + \".\" + v_config[\"table\"][\"source_schema\"] + \".\" + v_config[\"table\"][\"source_table\"] + \"|\" + v_config[\"table\"][\"destination_catalog\"] + \".\" + v_config[\"table\"][\"destination_schema\"] + \".\" + v_config[\"table\"][\"destination_table\"]\n",
    "            v_config_dict[v_id] = v_config\n",
    "\n",
    "        # Construct unique Ids for comparison - List of Tables Retrieved from Pipeline\n",
    "        for v_pipeline in p_pipeline_array:\n",
    "            v_id = v_pipeline[\"table\"][\"source_catalog\"] + \".\" + v_pipeline[\"table\"][\"source_schema\"] + \".\" + v_pipeline[\"table\"][\"source_table\"] + \"|\" + v_pipeline[\"table\"][\"destination_catalog\"] + \".\" + v_pipeline[\"table\"][\"destination_schema\"] + \".\" + (v_pipeline[\"table\"][\"destination_table\"] if 'destination_table' in v_pipeline[\"table\"] else v_pipeline[\"table\"][\"source_table\"]) \n",
    "            v_pipeline_dict[v_id] = v_pipeline\n",
    "\n",
    "        # Construct unique Ids for comparison - List of Tables Retrieved for Full Refresh from Config\n",
    "        for v_config in p_config_full_refresh_array:\n",
    "            v_id = v_config[\"table\"][\"source_catalog\"] + \".\" + v_config[\"table\"][\"source_schema\"] + \".\" + v_config[\"table\"][\"source_table\"] + \"|\" + v_config[\"table\"][\"destination_catalog\"] + \".\" + v_config[\"table\"][\"destination_schema\"] + \".\" + v_config[\"table\"][\"destination_table\"]\n",
    "            v_config_full_refresh_dict[v_id] = v_config\n",
    "\n",
    "        # Loop through list of tables from Pipeline and get what needs to be removed, fully refreshed and incremental refreshed\n",
    "        for v_id in v_pipeline_dict:\n",
    "            if v_id not in v_config_dict:\n",
    "                v_return_dict[\"removed\"].append(v_pipeline_dict[v_id])\n",
    "            elif v_id in v_config_dict:\n",
    "                v_return_dict[\"unchanged\"].append(v_config_dict[v_id])\n",
    "                if v_id in v_config_full_refresh_dict:\n",
    "                    v_return_dict[\"full_refresh\"].append(v_config_dict[v_id])\n",
    "                else:\n",
    "                    v_return_dict[\"refresh\"].append(v_config_dict[v_id])  \n",
    "\n",
    "        # Loop through list of tables from Config and get what needs to be added\n",
    "        for v_id in v_config_dict:\n",
    "            if v_id not in v_pipeline_dict:\n",
    "                v_return_dict[\"added\"].append(v_config_dict[v_id])\n",
    "\n",
    "        # Collect Counts for decision making during processing\n",
    "        v_return_dict[\"added_count\"] = len(v_return_dict[\"added\"])\n",
    "        v_return_dict[\"removed_count\"] = len(v_return_dict[\"removed\"])\n",
    "        v_return_dict[\"unchanged_count\"] = len(v_return_dict[\"unchanged\"])\n",
    "        v_return_dict[\"full_refresh_count\"] = len(v_return_dict[\"full_refresh\"])\n",
    "        v_return_dict[\"refresh_count\"] = len(v_return_dict[\"refresh\"])\n",
    "        v_return_dict[\"config_count\"] = len(v_config_dict)\n",
    "        v_return_dict[\"pipeline_count\"] = len(v_pipeline_dict)       \n",
    "\n",
    "        # Return Counts and Lists\n",
    "        return v_return_dict\n",
    "    \n",
    "    def build_config_source_table_array(self, p_row_dict, isHistorical=None):\n",
    "        # Build List of Tables for Incremental or Full Refresh based on IsHistorical Flag\n",
    "        return  [\n",
    "                    {\n",
    "                        \"table\": {\n",
    "                            \"source_catalog\": p_row_dict['SourceDatabaseName1'],\n",
    "                            \"source_schema\": table[\"SourceSchema\"],\n",
    "                            \"source_table\": table[\"SourceTable\"],\n",
    "                            \"destination_catalog\": p_row_dict['DestinationCatalog'],\n",
    "                            \"destination_schema\": p_row_dict['DestinationSchema'],\n",
    "                            \"destination_table\": table[\"DestinationTable\"] \n",
    "                        }\n",
    "                    }\n",
    "                    for table in json.loads(p_row_dict['TableList'])[\"data\"] if isHistorical is None or table['IsHistorical'] == isHistorical\n",
    "                ]\n",
    "                                         \n",
    "    def get_api_obj(self):\n",
    "        return LakeflowAPI()\n",
    "    \n",
    "    def get_process_config_object(self):\n",
    "        return ProcessConfigData(self.params_dict['p_environment'])\n",
    "    \n",
    "    def set_pipeline_process_status(self, p_config_row_dict, p_status):\n",
    "        v_process_status_dict = {   'PipelineName': p_config_row_dict['IngestionPipelineName'], \\\n",
    "                                    'InternalProductId' : p_config_row_dict['InternalProductId'], \\\n",
    "                                    'DataSourceId' : p_config_row_dict['DataSourceId'], \\\n",
    "                                    'StepType' : 'Extract', \\\n",
    "                                    'Status': p_status\n",
    "                                }\n",
    "        process_config = self.get_process_config_object()\n",
    "        process_config.set_pipeline_process_status(v_process_status_dict)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "OrchestratorHandler",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
