{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf9cd10-95fd-46f1-b9af-6c25c967cfee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import traceback\n",
    "from pyspark.sql.functions import collect_list, concat_ws, expr, col, length, lit, concat, lower, when, round, trim\n",
    "from pyspark.sql.types import StructType,StructField, StringType, LongType\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a616ebd3-64b9-4405-898e-885a2880df7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./UtilityHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507a12c9-410f-430c-bab2-adb62f0bd0ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./GlobalVariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22cb543-999b-4cd7-bdc3-79f7dd5eed77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./ProcessConfigHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66079c4b-86de-42be-84ce-82a3b5a268e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class LakeflowCountReconciliation():\n",
    "    def __init__(self, p_environment,  p_internal_product_id, p_source_server_name, p_source_database_name=None, p_internal_client_id=None, p_internal_facility_id=None,p_ingestion_pipeline_name=None):\n",
    "        self.params_dict = {\n",
    "            'p_environment': p_environment,\n",
    "            'p_internal_product_id': p_internal_product_id,\n",
    "            'p_source_server_name': p_source_server_name,\n",
    "            'p_source_database_name': p_source_database_name,\n",
    "            'p_internal_client_id': p_internal_client_id,\n",
    "            'p_internal_facility_id': p_internal_facility_id,\n",
    "            'p_ingestion_pipeline_name': p_ingestion_pipeline_name\n",
    "        }\n",
    "\n",
    "        self.process_max_workers = 10\n",
    "        self.trackeback_length = 1000\n",
    "        self.saveas_table = 'db_config.ProcessReconciliationCounts'\n",
    "\n",
    "    def set_process_max_workers(self, process_max_workers):\n",
    "        self.process_max_workers = process_max_workers\n",
    "\n",
    "    def get_process_max_workers(self):\n",
    "        return self.process_max_workers\n",
    "    \n",
    "    def set_traceback_length(self, traceback_length):\n",
    "        self.trackeback_length = traceback_length\n",
    "\n",
    "    def get_traceback_length(self):\n",
    "        return self.trackeback_length\n",
    "    \n",
    "    def set_saveas_table(self, saveas_table):\n",
    "        self.saveas_table = saveas_table\n",
    "\n",
    "    def get_saveas_table(self):\n",
    "        return self.saveas_table\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        v_return_dict = {'Total': 0, 'Success': 0, 'SuccessResults': [], 'ExecutionFailed' : 0, 'ExecutionFailures': []}\n",
    "        v_thread_errors = []       # To collect all exceptions\n",
    "        v_thread_results = []      # To collect successful results\n",
    "        v_row_array = []\n",
    "        \n",
    "         # Retrieve List of Source and Destination Systems \n",
    "        process_config = ProcessConfigData(self.params_dict[\"p_environment\"])\n",
    "        df_config = process_config.get_table_list_aggregate (\n",
    "                                                                self.params_dict[\"p_internal_product_id\"], \n",
    "                                                                self.params_dict[\"p_source_server_name\"], \n",
    "                                                                self.params_dict[\"p_source_database_name\"], \n",
    "                                                                self.params_dict[\"p_internal_client_id\"], \n",
    "                                                                self.params_dict[\"p_internal_facility_id\"],\n",
    "                                                                self.params_dict['p_ingestion_pipeline_name']\n",
    "                                                            )\n",
    "        \n",
    "        df_existing = df_config.filter((col(\"IngestionPipelineId\").isNotNull()) & (trim(col(\"IngestionPipelineId\")) != \"\"))\n",
    "\n",
    "        # Get Unitity Catalog and Managed Location Root Path from Process Config Attributes\n",
    "        v_unity_catalog = process_config.get_config_attribute_value('AnalyticsUnityCatalog')\n",
    "        v_managed_location_root_path = process_config.get_config_attribute_value('AdlsAnalyticsFullpathUri')\n",
    "\n",
    "        # Construct Row Array for Processing Existing Pipelines and adding common Attributes\n",
    "        for row in df_existing.collect():\n",
    "            v_row_dict = row.asDict()\n",
    "            v_row_dict['Status'] = 'Pending'\n",
    "            v_row_dict['SourceType'] = 'SqlServer'\n",
    "            v_row_dict['DestinationType'] = 'UnityCatalog'\n",
    "            v_row_dict['DestinationCatalog'] = v_unity_catalog\n",
    "            v_row_dict['ManagedLocationRootPath'] = v_managed_location_root_path\n",
    "            v_row_dict['process_config'] = process_config\n",
    "            v_row_dict['ProcessIdentifier'] = f\"Source={v_row_dict['SourceServerName1']}/{v_row_dict['SourceDatabaseName1']} <-> Destination={v_row_dict['DestinationCatalog']}/{v_row_dict['DestinationSchema']}\"\n",
    "            v_row_dict['Params'] = self.params_dict\n",
    "            v_row_array.append(v_row_dict)\n",
    "\n",
    "        # Launch Threads for Process Existing Pipelines in parallell\n",
    "        with ThreadPoolExecutor(max_workers=self.get_process_max_workers()) as executor:\n",
    "            futures = {executor.submit(self.process_row, obj): obj for obj in v_row_array}\n",
    "            for future in as_completed(futures):\n",
    "                obj = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    v_thread_results.append(str(obj['ProcessIdentifier']) + ' \\nMessage=Count Reconciliation Successfully retrieved - : ' + str(result))\n",
    "                    #print(f\"Thread Successes: \", v_thread_results)\n",
    "                except Exception as e:\n",
    "                    v_thread_errors.append(str(obj['ProcessIdentifier']) + ' \\nMessage=' + str(e).replace('\\n','')[0:self.get_traceback_length()] + ' \\nTraceback=' + traceback.format_exc().replace('\\n','')[0:self.get_traceback_length()] )\n",
    "                    #print(f\"Thread Errors: \", v_thread_errors)\n",
    "\n",
    "        # Collect Statistics for Summary\n",
    "        v_return_dict['Total'] = len(v_row_array)\n",
    "        v_return_dict['Success'] = len(v_thread_results)\n",
    "        v_return_dict['SuccessResults'] = v_thread_results\n",
    "        v_return_dict['ExecutionFailed'] = len(v_thread_errors)\n",
    "        v_return_dict['ExecutionFailures'] = v_thread_errors\n",
    "\n",
    "        print(\"\\n\\nSummary:\\n\")\n",
    "        pprint.pprint(v_return_dict, indent=4, compact=True, sort_dicts=False, width=10000)\n",
    "        # Raise Exception if error or nothing to process\n",
    "        if len(v_thread_errors) > 0:\n",
    "            raise Exception(f\"{len(v_thread_errors)} Execution errors were encountered during processing. See exception(s) for details.\")\n",
    "        elif len(v_row_array) == 0:\n",
    "            raise Exception(f\"Unable to find valid configuration(s) for Parameters: {self.params_dict}\")\n",
    "        else:\n",
    "            print(f\"Processing Complete. {len(v_thread_results)} of {len(v_row_array)} Reconciliation Tasks were processed successfully.\\n\\n\")\n",
    "        \n",
    "        return v_return_dict\n",
    "    \n",
    "    def process_row(self, p_row_dict):\n",
    "        obj_recon = CountReconciliation(p_row_dict['Params']['p_environment'],\n",
    "                                                   p_row_dict['SourceType'],\n",
    "                                                   p_row_dict['SourceServerName1'],\n",
    "                                                   p_row_dict['SourceDatabaseName1'],\n",
    "                                                   p_row_dict['DataSourceShortName'],\n",
    "                                                   p_row_dict['DestinationType'],\n",
    "                                                   p_row_dict['DestinationCatalog'],\n",
    "                                                   p_row_dict['DestinationSchema'])\n",
    "        df_result = obj_recon.process()\n",
    "\n",
    "        if df_result and df_result.count() > 0:\n",
    "            df_result.write.mode('append').saveAsTable(self.get_saveas_table())\n",
    "\n",
    "        return df_result.count() if df_result else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d93db02-ee5a-4bbe-b599-dfd9373db8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CountReconciliation():\n",
    "    def __init__(self, p_environment, p_source_type, p_source_server_name, p_source_database_name, p_source_short_name, p_destination_type,  p_destination_catalog, p_destination_schema):\n",
    "        self.params_dict = {\n",
    "            'p_environment': p_environment,\n",
    "            'p_source_type': p_source_type,\n",
    "            'p_source_server_name': p_source_server_name,\n",
    "            'p_source_database_name': p_source_database_name,\n",
    "            'p_source_short_name': p_source_short_name,\n",
    "            'p_destination_type': p_destination_type,\n",
    "            'p_destination_catalog': p_destination_catalog,\n",
    "            'p_destination_schema': p_destination_schema\n",
    "        }\n",
    "        gv_obj = GlobalVars(p_environment)\n",
    "        self.dbx_scope = gv_obj.get_database_config('dbx_scope')\n",
    "        \n",
    "\n",
    "    def get_sqlsvr_counts(self, p_source_type, p_source_server_name, p_source_database_name, p_source_short_name):\n",
    "\n",
    "        jdbcUsername = \"cdc_analytics\"\n",
    "        jdbcPassword = dbutils.secrets.get(scope = self.dbx_scope, key = \"EtlSqlCDCSecret\")\n",
    "        jdbcHostname = f\"{p_source_server_name}.NTHRIVE.NTHCRP.COM\" if '.com' not in p_source_server_name.lower() else p_source_server_name\n",
    "        jdbcPort = 1433\n",
    "        jdbcDatabase = p_source_database_name\n",
    "        jdbcHostNamePort = f\"{jdbcHostname}:{jdbcPort}\"\n",
    "\n",
    "        jdbcQuery = f\"\"\"select distinct s.name as SourceSchema, t.name as SourceTable, p.rows as SourceCount\n",
    "                        from sys.partitions p\n",
    "                        join sys.tables t on t.object_id = p.object_id\n",
    "                        join sys.schemas s on t.schema_id = s.schema_id\n",
    "                        where t.is_tracked_by_cdc = 1\n",
    "                        \"\"\"\n",
    "        #print(jdbcQuery)\n",
    "\n",
    "        df_remote_query = (spark.read\n",
    "        .format(\"sqlserver\")\n",
    "        .option(\"host\", jdbcHostname)\n",
    "        .option(\"port\", jdbcPort) # optional, can use default port 1433 if omitted\n",
    "        .option(\"user\", jdbcUsername)\n",
    "        .option(\"password\", jdbcPassword)\n",
    "        .option(\"database\", jdbcDatabase)\n",
    "        #.option(\"authentication\", \"ActiveDirectoryPassword\")\n",
    "        .option(\"trustServerCertificate\",\"true\")\n",
    "        .option(\"hostNameInCertificate\",\"*.NTHRIVE.NTHCRP.COM\")\n",
    "        .option(\"query\", jdbcQuery) # (if schemaName not provided, default to \"dbo\")\n",
    "        .load()) \\\n",
    "        .withColumn(\"SourceType\", lit(p_source_type)) \\\n",
    "        .withColumn(\"SourceServerName1\", lit(p_source_server_name)) \\\n",
    "        .withColumn(\"SourceDatabaseName1\", lit(p_source_database_name)) \\\n",
    "        .withColumn('tbl', concat(lit(p_source_short_name), lit('_ods_'), lower(col('SourceTable')))  ) \\\n",
    "        .orderBy('tbl')\n",
    "\n",
    "        #df_remote_query.display()\n",
    "\n",
    "        return df_remote_query\n",
    "    \n",
    "    def get_destination_streaming_counts(self, p_destination_type, p_destination_catalog, p_destination_schema, p_source_short_name):\n",
    "        # Construct SQL Count Statement for each table\n",
    "        sql_text = f\"\"\"\n",
    "            select \"select \"  || \"'\" || table_name || \"'\" || ' as DestinationTable, count(*) as DestinationCount from ' || table_catalog || '.' || table_schema || '.' || table_name || ' union ' as value\n",
    "            from {p_destination_catalog}.information_schema.tables where table_schema = '{p_destination_schema}' and table_catalog = '{p_destination_catalog}' and table_name not like '__%' escape '_'\n",
    "            order by 1\n",
    "        \"\"\"\n",
    "        \n",
    "        # Run Construct Statement to get table list\n",
    "        df_stream_list = spark.sql(sql_text)\n",
    "        \n",
    "        # Combine all separate statements into one statement\n",
    "        df_stream_statement = df_stream_list \\\n",
    "                    .agg(collect_list(\"value\").alias(\"combined_values\")) \\\n",
    "                    .withColumn(\"combined_values\", concat_ws(\" \", \"combined_values\"))  \n",
    "    \n",
    "        #df_stream_statement.display()\n",
    "\n",
    "        if df_stream_statement.count() > 0:\n",
    "            # Get the combined statement as text\n",
    "            sql_result_text = df_stream_statement.collect()[0]['combined_values']\n",
    "            sql_result_text =  StringUtils.remove_last_occurrence(sql_result_text, ' union ') + ' order by 1 ' # sql_result_text.rstrip(' union ')\n",
    "\n",
    "            # Calculate Counts\n",
    "            df_result = spark.sql(sql_result_text) \\\n",
    "                        .withColumn(\"DestinationType\", lit(p_destination_type)) \\\n",
    "                        .withColumn(\"DestinationCatalog\", lit(p_destination_catalog)) \\\n",
    "                        .withColumn(\"DestinationSchema\", lit(p_destination_schema)) \\\n",
    "                        .withColumn('tbl', concat(lit(p_source_short_name), lit('_ods_'), col('DestinationTable'))  )  \n",
    "            #df_result.show(100, truncate=False)\n",
    "            #print(datetime.now(), df_result.count())\n",
    "\n",
    "        return df_result\n",
    "    \n",
    "    def get_recon_source_stream(self, p_source_type, p_source_server_name, p_source_database_name, p_source_short_name, p_destination_type, p_destination_catalog, p_destination_schema):\n",
    "        # Get Counts on Source and Stream\n",
    "        df_remote = self.get_sqlsvr_counts(p_source_type, p_source_server_name, p_source_database_name, p_source_short_name)\n",
    "        df_stream = self.get_destination_streaming_counts(p_destination_type, p_destination_catalog, p_destination_schema, p_source_short_name)\n",
    "        # Get Report Date Information\n",
    "        report_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        report_hour = datetime.now().strftime(\"%H\")\n",
    "        report_minute = datetime.now().strftime(\"%M\")\n",
    "        # Join Source and Stream\n",
    "        df_recon = df_remote.join(df_stream, ['tbl'], 'fullouter') \\\n",
    "                    .withColumn('DiffCount', round(col('SourceCount') - col('DestinationCount'), 0)) \\\n",
    "                    .withColumn('DiffPercentage', round(col('DiffCount') / col('SourceCount') * 100, 2)) \\\n",
    "                    .withColumn('ReportDate', lit(report_date)) \\\n",
    "                    .withColumn('ReportHour', lit(report_hour)) \\\n",
    "                    .withColumn('ReportMinute', lit(report_minute))\n",
    "\n",
    "        df_recon.display()\n",
    "        return df_recon\n",
    "    \n",
    "    def process(self):\n",
    "        return self.get_recon_source_stream(self.params_dict['p_source_type'],\n",
    "                                                self.params_dict['p_source_server_name'], \n",
    "                                                self.params_dict['p_source_database_name'], \n",
    "                                                self.params_dict['p_source_short_name'], \n",
    "                                                self.params_dict['p_destination_type'],\n",
    "                                                self.params_dict['p_destination_catalog'], \n",
    "                                                self.params_dict['p_destination_schema'])\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cab4e487-2bf8-48aa-88cc-abdf95dd4810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# obj = LakeflowCountReconciliation('dev', 27,'lewvpalyedb04.nthext.com', None, None, None,'ingst_sqlcdc_global_lewvpalyedb04_100001')\n",
    "# obj.process()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CountReconciliationHandler",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
