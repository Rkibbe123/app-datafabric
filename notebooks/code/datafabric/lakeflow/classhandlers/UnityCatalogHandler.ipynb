{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f9cab8-2374-4771-8e32-b33c7218d4cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Utility Classes"
    }
   },
   "outputs": [],
   "source": [
    "%run ../classhandlers/UtilityHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96866642-7673-450b-b3bc-6849bcc23050",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UnityCatalog"
    }
   },
   "outputs": [],
   "source": [
    "class UnityCatalog:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_schema_fullname(self,  p_full_schema_name, p_schema_path=None):\n",
    "        v_location_path = f\"MANAGED LOCATION '{p_schema_path}'\" if p_schema_path else \"\"\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {p_full_schema_name} {v_location_path};\")\n",
    "        return p_full_schema_name\n",
    "    \n",
    "    def create_schema(self, p_catalog_name, p_schema_name, p_schema_path=None):\n",
    "        v_full_schema_name = f\"{p_catalog_name}.{p_schema_name}\"\n",
    "        return self.create_schema_fullname(v_full_schema_name, p_schema_path)\n",
    "    \n",
    "    def create_table_like (self, p_table_full_name, p_table_like_full_name, p_partition_column_list=[]):\n",
    "        sql_text = f\"CREATE TABLE IF NOT EXISTS {p_table_full_name}\"\n",
    "        if len(p_partition_column_list) > 0:\n",
    "            sql_text += f\" PARTITIONED BY ({','.join(p_partition_column_list)})\"\n",
    "        sql_text += \"  AS SELECT * FROM {p_table_like_full_name} where 1=0;\"\n",
    "        spark.sql(sql_text)\n",
    "        return p_table_full_name\n",
    "    \n",
    "    def get_default_catalog(self):\n",
    "        return spark.sql(\"SELECT current_catalog() as catalog_name\").collect()[0][0]\n",
    "    \n",
    "    def get_table_column_names(self,p_table_full_name):\n",
    "        return spark.sql(f\"select * from {p_table_full_name} LIMIT 0;\").columns if spark.catalog.tableExists(p_table_full_name) else []\n",
    "    \n",
    "    def table_add_cluster_key(self, p_table_full_name, p_cluster_column_list=None):\n",
    "        if p_cluster_column_list:\n",
    "            spark.sql(f\"ALTER TABLE {p_table_full_name} CLUSTER BY ({','.join(p_cluster_column_list)});\")\n",
    "        else:\n",
    "            spark.sql(f\"ALTER TABLE {p_table_full_name} CLUSTER BY AUTO;\")\n",
    "        return p_table_full_name\n",
    "    \n",
    "    def table_add_primary_key(self, p_table_full_name, p_primary_key_column_list, p_rely=False):\n",
    "        # Set Primary Columns to NOT NULL\n",
    "        for col in p_primary_key_column_list:\n",
    "            spark.sql(f\"ALTER TABLE {p_table_full_name} ALTER COLUMN {col} SET NOT NULL;\")\n",
    "        # Add list of columns as Primary Key(s)\n",
    "        sql_text = f\"ALTER TABLE {p_table_full_name} ADD PRIMARY KEY ({','.join(p_primary_key_column_list)})\"\n",
    "        # Add rely clause if p_rely is True\n",
    "        sql_text += \" RELY\" if p_rely else \"\"\n",
    "        # Execute Alter statement\n",
    "        spark.sql(f\"{sql_text};\")\n",
    "\n",
    "        return p_table_full_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25f7dfb5-7852-4460-9db9-921e7e3b9274",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UnityCatalogTableExtension"
    }
   },
   "outputs": [],
   "source": [
    "class UnityCatalogTableExtension(UnityCatalog):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def create_table_union_sql_text(self, p_catalog, p_schema_like, p_table_like, except_columns_list=[]):\n",
    "        sql_result_text = None\n",
    "        except_select_text = ''\n",
    "        if len(except_columns_list) > 0:\n",
    "            except_select_text = ', '.join(except_columns_list)\n",
    "            except_select_text = except_select_text.rstrip(', ')\n",
    "            except_select_text = f\" except ({except_select_text})\"\n",
    "    \n",
    "        sql_text = f\"\"\"select concat(\"select * {except_select_text} from \", table_catalog,'.',table_schema,'.',table_name,' union') as value\n",
    "                        from {p_catalog}.information_schema.tables where table_catalog = '{p_catalog}' and table_schema ilike '{p_schema_like}' and table_name ilike '{p_table_like}'\"\"\"\n",
    "\n",
    "        # Strip last 'union' text\n",
    "        sql_text = sql_text.rstrip('union')\n",
    "\n",
    "        # Run Construct Statement to get table list\n",
    "        df_table_list = spark.sql(sql_text)\n",
    "\n",
    "        # Combine all separate statements into one statement\n",
    "        df_combine_statement = df_table_list \\\n",
    "                    .agg(collect_list(\"value\").alias(\"combined_values\")) \\\n",
    "                    .withColumn(\"combined_values\", concat_ws(\" \", \"combined_values\"))  \n",
    "\n",
    "        if df_combine_statement.count() > 0:\n",
    "            # Get the combined statement as text\n",
    "            sql_result_text = df_combine_statement.collect()[0]['combined_values']\n",
    "            sql_result_text = sql_result_text.rstrip(' union ')\n",
    "        \n",
    "        return sql_result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b803d7-f91e-4aa8-a324-b21be2c70c17",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UnityCatalogTableOperations"
    }
   },
   "outputs": [],
   "source": [
    "class UnityCatalogTableOperations(UnityCatalog):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def merge_table(self, p_source_table, p_target_table, p_source_filter=None, p_keys=None, p_create_audit_columns_dict={}, p_update_audit_columns_dict={}):\n",
    "        \n",
    "        p_keys = [] if p_keys is None else [k.strip() for k in p_keys]\n",
    "        p_create_audit_columns_dict = {} if p_create_audit_columns_dict is None else p_create_audit_columns_dict\n",
    "        p_update_audit_columns_dict = {} if p_update_audit_columns_dict is None else p_update_audit_columns_dict\n",
    "        p_source_filter = \"\" if p_source_filter is None else f\"WHERE {p_source_filter}\"\n",
    "\n",
    "        return_dict = {'status': 'failed'}\n",
    "        \n",
    "        #check if tables exist\n",
    "        if spark.catalog.tableExists(p_source_table) \\\n",
    "            and spark.catalog.tableExists(p_target_table):\n",
    "            #Get Source and Target Column Lists\n",
    "            v_source_column_list = self.get_table_column_names(p_source_table)\n",
    "            v_target_column_list = self.get_table_column_names(p_target_table)\n",
    "            \n",
    "            #Find Common Columns and Remove Audit Columns\n",
    "            v_common_columns = list(set(v_source_column_list).intersection(v_target_column_list))\n",
    "            v_common_columns = list(set(v_common_columns).difference(p_create_audit_columns_dict.keys()))\n",
    "            v_common_columns = list(set(v_common_columns).difference(p_update_audit_columns_dict.keys()))\n",
    "\n",
    "            # Remove key Columns for Update\n",
    "            v_common_columns_update = list(set(v_common_columns).difference(p_keys))\n",
    "\n",
    "            # Convert Lists to Dictionaries and Add Audit Columns for Create and Update\n",
    "            v_common_columns_dict = {column: column for column in v_common_columns}\n",
    "            v_common_columns_update_dict = {f\"target.{column}\": f\"source.{column}\" for column in v_common_columns_update}\n",
    "            v_common_columns_dict.update(p_create_audit_columns_dict)\n",
    "            v_common_columns_update_dict.update(p_update_audit_columns_dict)\n",
    "\n",
    "            # Build Source Select Statement with Filter if passed\n",
    "            v_source_table_select_str = 'select ' + ', '.join(v_source_column_list) + ' from {0} {1}'.format(p_source_table, p_source_filter)\n",
    "\n",
    "            # Check if audit columns are in target and keys are in common columns\n",
    "            if  all(elem in set(v_target_column_list) for elem in p_create_audit_columns_dict.keys()) and \\\n",
    "                all(elem in set(v_target_column_list) for elem in p_update_audit_columns_dict.keys()) and \\\n",
    "                len(p_keys) > 0 and \\\n",
    "                all(elem in set(v_common_columns) for elem in p_keys):\n",
    "                    \n",
    "                # Build Merge Template\n",
    "                merge_sql_str = \"Merge into {0} as target using ({1}) as source on {2} when matched then update set {3} when not matched then insert ({4}) values ({5})\"\n",
    "\n",
    "                # Subtitute Template Values\n",
    "                merge_sql_str = merge_sql_str.format(p_target_table, \n",
    "                                                    v_source_table_select_str, \n",
    "                                                    ' and '.join([f\"target.{column} = source.{column}\" for column in p_keys]), \n",
    "                                                    ', '.join([f\"{k} = {v}\" for k,v in v_common_columns_update_dict.items()]), \n",
    "                                                    ', '.join(v_common_columns_dict.keys()), \n",
    "                                                    ', '.join(v_common_columns_dict.values())\n",
    "                                                )\n",
    "            else:\n",
    "                raise Exception(\"Audit columns not in Target or Keys are not in either Source or Target. Keys are case-sensitive. Check Configuration.\")\n",
    "            \n",
    "            # Execute Merge Statement\n",
    "            #print(\"\\n\\n\",merge_sql_str,\"\\n\\n\")\n",
    "            df_merge = spark.sql(merge_sql_str)\n",
    "\n",
    "            # Collect Merge metrics\n",
    "            df_merge_result = df_merge.collect()\n",
    "            if len(df_merge_result) > 0:\n",
    "                return_dict['num_affected_rows'] = df_merge_result[0]['num_affected_rows']\n",
    "                return_dict['num_updated_rows'] = df_merge_result[0]['num_updated_rows']\n",
    "                return_dict['num_inserted_rows'] = df_merge_result[0]['num_inserted_rows']\n",
    "                return_dict['num_deleted_rows'] = df_merge_result[0]['num_deleted_rows']\n",
    "            return_dict['status'] = 'success'\n",
    "        else:\n",
    "            raise Exception(\"Source or Target Table does not exist\")\n",
    "\n",
    "        return return_dict\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "UnityCatalogHandler",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
