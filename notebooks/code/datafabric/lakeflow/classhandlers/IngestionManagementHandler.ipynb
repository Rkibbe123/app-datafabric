{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39e99d8-0f00-4c31-80f1-bc043e013e83",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Process Config Classes"
    }
   },
   "outputs": [],
   "source": [
    "%run ./ProcessConfigHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd2728f-1f46-4494-a4b9-711b4adf4ec6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Lakeflow API Classes"
    }
   },
   "outputs": [],
   "source": [
    "%run ./LakeflowAPIHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "387119dc-a7e6-4c29-b9a4-37d981703f58",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Python Modules"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import traceback\n",
    "import threading\n",
    "import multiprocessing\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2a0c38-91d6-4e8d-9470-6c60564081d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SqlIngestionCommon"
    }
   },
   "outputs": [],
   "source": [
    "class SqlIngestionCommon:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def start_gateway_pipeline(self, p_row_dict, api_obj=None):\n",
    "\n",
    "        v_state = None\n",
    "        v_retry_max = 20\n",
    "        v_retry_delay_seconds = 60\n",
    "        v_retry_count = 0\n",
    "\n",
    "        print(f\"Starting Gateway Pipeline {p_row_dict['GatewayPipelineName']}....\")\n",
    "        # Retrieve Gateway Current Information\n",
    "        api_obj = LakeflowAPI() if api_obj is None else api_obj\n",
    "        v_pipeline_dict = api_obj.get_pipeline(p_row_dict['GatewayPipelineId'])\n",
    "\n",
    "        if v_pipeline_dict['status'] == 'ok':\n",
    "            v_state = v_pipeline_dict['latest_update_state']\n",
    "            if v_state != 'RUNNING':\n",
    "                # Set Cluster Specs based on Historical Flag at the Gateway Level\n",
    "                v_clusters_list = json.loads(p_row_dict['ClusterJSONSpecsLarge']) if p_row_dict['IsHistoricalGatewayLevel'] == 1 else json.loads(p_row_dict['ClusterJSONSpecsSmall'])\n",
    "                # Update Cluster Specs and Start Gateway\n",
    "                gw_mng_obj = UpdateSqlGatewayPipeline(p_row_dict['GatewayPipelineId'])\n",
    "                v_pipeline_start_dict = gw_mng_obj.update_pipeline_clusters(v_clusters_list[\"clusters\"]) \n",
    "                # Wait for Gateway to be in Running State\n",
    "                v_retry_count = 0\n",
    "                while v_state != 'RUNNING' and v_retry_count < v_retry_max:\n",
    "                    time.sleep(v_retry_delay_seconds)\n",
    "                    v_pipeline_dict = api_obj.get_pipeline(p_row_dict['GatewayPipelineId'])\n",
    "                    v_state = v_pipeline_dict['latest_update_state']\n",
    "                    v_retry_count += 1\n",
    "\n",
    "        if v_state != \"RUNNING\":\n",
    "            raise Exception (f\"Error: Gateway Pipeline {p_row_dict['GatewayPipelineName']} failed to start - State: {v_state}\")\n",
    "        else:\n",
    "            print(f\"Gateway Pipeline {p_row_dict['GatewayPipelineName']} in RUNNIG state....\")\n",
    "\n",
    "        return v_state\n",
    "\n",
    "    def stop_gateway_pipeline(self, p_row_dict, api_obj=None):\n",
    "\n",
    "        v_state = None\n",
    "        v_retry_max = 120\n",
    "        v_retry_delay_seconds = 60\n",
    "        v_retry_count = 0\n",
    "        v_ingestion_last_update_state = None\n",
    "\n",
    "        print(f\"Shutting down Gateway {p_row_dict['GatewayPipelineName']} when Ingestion {p_row_dict['IngestionPipelineName']} is IDLE....\")\n",
    "        api_obj = LakeflowAPI() if api_obj is None else api_obj\n",
    "        \n",
    "        time.sleep(20)\n",
    "        # Check if Ingestion Pipeline no longer running before stoppping gateway\n",
    "        v_pipeline_dict = api_obj.get_pipeline(p_row_dict['IngestionPipelineId'])\n",
    "        \n",
    "        if v_pipeline_dict['status'] == 'ok':\n",
    "            v_state = v_pipeline_dict['state']\n",
    "            v_ingestion_last_update_state = v_pipeline_dict['latest_update_state']\n",
    "            v_retry_count = 0\n",
    "            while v_state != 'IDLE' and v_retry_count < v_retry_max:\n",
    "                time.sleep(v_retry_delay_seconds)\n",
    "                v_pipeline_dict = api_obj.get_pipeline(p_row_dict['IngestionPipelineId'])\n",
    "                v_state = v_pipeline_dict['state']\n",
    "                v_ingestion_last_update_state = v_pipeline_dict['latest_update_state']\n",
    "                v_retry_count += 1\n",
    "        \n",
    "        # Set Process Status to Fail/Success\n",
    "        if v_ingestion_last_update_state == 'COMPLETED':\n",
    "            self.set_pipeline_process_status(p_row_dict, 'C')\n",
    "        else:\n",
    "            self.set_pipeline_process_status(p_row_dict, 'F')\n",
    "\n",
    "        # Sop Gateway Pipeline\n",
    "        if v_state is not None and v_state == 'IDLE':\n",
    "            v_pipeline_dict = api_obj.get_pipeline(p_row_dict['GatewayPipelineId'])\n",
    "            if v_pipeline_dict['status'] == 'ok':\n",
    "                v_state = v_pipeline_dict['latest_update_state']\n",
    "                if v_state == 'RUNNING':\n",
    "                    v_pipeline_start_dict = api_obj.stop_pipeline(p_row_dict['GatewayPipelineId'])\n",
    "                    v_retry_count = 0\n",
    "                    while v_state == 'RUNNING' and v_retry_count < v_retry_max:\n",
    "                        time.sleep(v_retry_delay_seconds)\n",
    "                        v_pipeline_dict = api_obj.get_pipeline(p_row_dict['GatewayPipelineId'])\n",
    "                        v_state = v_pipeline_dict['latest_update_state']\n",
    "                        v_retry_count += 1 \n",
    "\n",
    "        return {'GatewayState': v_state, 'IngestionState': v_ingestion_last_update_state}\n",
    "    \n",
    "    def wait_for_ingestion_pipeline_idle(self, p_row_dict, api_obj=None):\n",
    "        v_state = None\n",
    "        v_ingestion_last_update_state = None\n",
    "        v_retry_max = 120\n",
    "        v_retry_delay_seconds = 60\n",
    "        v_retry_count = 0\n",
    "\n",
    "        time.sleep(20)\n",
    "        # Check if Ingestion Pipeline no longer running\n",
    "        v_pipeline_dict = api_obj.get_pipeline(p_row_dict['IngestionPipelineId'])\n",
    "        \n",
    "        if v_pipeline_dict['status'] == 'ok':\n",
    "            v_state = v_pipeline_dict['state']\n",
    "            v_ingestion_last_update_state = v_pipeline_dict['latest_update_state']\n",
    "            v_retry_count = 0\n",
    "            while v_state != 'IDLE' and v_retry_count < v_retry_max:\n",
    "                time.sleep(v_retry_delay_seconds)\n",
    "                v_pipeline_dict = api_obj.get_pipeline(p_row_dict['IngestionPipelineId'])\n",
    "                v_state = v_pipeline_dict['state']\n",
    "                v_ingestion_last_update_state = v_pipeline_dict['latest_update_state']\n",
    "                v_retry_count += 1\n",
    "        \n",
    "        return v_ingestion_last_update_state\n",
    "\n",
    "    def delay_pipeline_start(self, p_row_dict, api_obj=None):\n",
    "        # Delay before Starting Pipeline based on Configuration\n",
    "        print(f\"Waiting {p_row_dict['IngestionDelayStartinSeconds']} seconds before starting Ingestion Pipeline {p_row_dict['IngestionPipelineName']}....\")\n",
    "        time.sleep(p_row_dict['IngestionDelayStartinSeconds'])\n",
    "    \n",
    "    def get_snapshot_startime(self):\n",
    "        # Snapshots Start Time in UTC to watch for Ingestion Pipeline\n",
    "        #Current time in UTC\n",
    "        now_utc = datetime.now(timezone.utc)\n",
    "        # substract 5 minutes\n",
    "        now_utc = now_utc - timedelta(minutes=5)\n",
    "        # Format with timezone offset and 'Z'\n",
    "        return now_utc.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'  # ISO 8601 with milliseconds\n",
    "    \n",
    "    def wait_for_snapshots_complete(self, p_config_row_dict, p_expected_table_count=None, p_snapshots_start_time=None, api_obj=None):\n",
    "        # Initialize Parameters if not passed\n",
    "        p_expected_table_count = p_config_row_dict['TableCount'] if not p_expected_table_count else p_expected_table_count\n",
    "        p_snapshots_start_time = '2000-01-01T00:00:00.000Z' if not p_snapshots_start_time else p_snapshots_start_time\n",
    "        api_obj = LakeflowAPI() if api_obj is None else api_obj\n",
    "\n",
    "        # Instantiate API Extension\n",
    "        api_ext_obj = LakeflowAPIExtension()\n",
    "\n",
    "        # Wait for All Snaphots to complete for current Pipeline\n",
    "        # First use SQL Query\n",
    "        if not api_ext_obj.wait_for_snapshots_complete(p_config_row_dict['GatewayPipelineId'], p_config_row_dict['IngestionPipelineId'], p_expected_table_count, p_snapshots_start_time):\n",
    "            # If Failed, use API Events\n",
    "            if not api_ext_obj.wait_for_snapshots_complete_via_api(p_config_row_dict['GatewayPipelineId'], p_config_row_dict['IngestionPipelineId'], p_expected_table_count, p_snapshots_start_time):\n",
    "                # If Failed, use Config Delay\n",
    "                self.delay_pipeline_start(p_config_row_dict, api_obj)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "016d44ce-5547-4bc3-8881-2718916333ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CreateSqlIngestionPipeline"
    }
   },
   "outputs": [],
   "source": [
    "class CreateSqlIngestionPipeline(SqlIngestionCommon):\n",
    "    def __init__(self, p_environment,  p_internal_product_id, p_source_server_name, p_source_database_name=None, p_internal_client_id=None, p_internal_facility_id=None):\n",
    "        super().__init__()\n",
    "        self.params_dict = {\n",
    "            'p_environment': p_environment,\n",
    "            'p_internal_product_id': p_internal_product_id,\n",
    "            'p_source_server_name': p_source_server_name,\n",
    "            'p_source_database_name': p_source_database_name,\n",
    "            'p_internal_client_id': p_internal_client_id,\n",
    "            'p_internal_facility_id': p_internal_facility_id\n",
    "        }\n",
    "\n",
    "        self.process_max_workers = 10\n",
    "        self.trackeback_length = 1000\n",
    "        lock_manager = multiprocessing.Manager()\n",
    "        self.lock = lock_manager.Lock()\n",
    "\n",
    "    def set_process_max_workers(self, process_max_workers):\n",
    "        self.process_max_workers = process_max_workers\n",
    "\n",
    "    def get_process_max_workers(self):\n",
    "        return self.process_max_workers\n",
    "    \n",
    "    def set_traceback_length(self, traceback_length):\n",
    "        self.trackeback_length = traceback_length\n",
    "\n",
    "    def get_traceback_length(self):\n",
    "        return self.trackeback_length\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        v_thread_errors = []        # To collect all exceptions\n",
    "        v_thread_results = []       # To collect successful results\n",
    "        v_validation_errors = []    # To collect validation errors\n",
    "        v_skipped_results = []      # To collect skipped results\n",
    "        v_config_row_array = []     # To collect valid config rows to process\n",
    "        # Initialize Return Dictionary - will display results in that order using pprint\n",
    "        v_return_dict = {'Total': 0, 'Success': 0, 'SuccessResults': [], 'Skipped': 0, 'SkippedResults': [], 'ValidationFailed' : 0, 'ValidationFailures': [], 'ExecutionFailed' : 0, 'ExecutionFailures': []}\n",
    "        # Retrieve Unity Catalog from Configuration\n",
    "        process_config = ProcessConfigData(self.params_dict['p_environment'])\n",
    "        v_unity_catalog = process_config.get_config_attribute_value('AnalyticsUnityCatalog')\n",
    "        # Retrieve new pipelines Configuration\n",
    "        df_config_data_rows = process_config.get_table_list_aggregate(\n",
    "                                                                                            self.params_dict['p_internal_product_id'],\n",
    "                                                                                            self.params_dict['p_source_server_name'], \n",
    "                                                                                            self.params_dict['p_source_database_name'],\n",
    "                                                                                            self.params_dict['p_internal_client_id'],\n",
    "                                                                                            self.params_dict['p_internal_facility_id']\n",
    "                                                                                        )\n",
    "        # Raise exception if no configurations found\n",
    "        if df_config_data_rows is None or df_config_data_rows.count() == 0:\n",
    "            raise Exception(f\"Unable to find implementation configuration for {self.params_dict}\")\n",
    "        else:\n",
    "            v_return_dict['Total'] = df_config_data_rows.count()\n",
    "            print(\"Pipeline Configuration Rows:\", df_config_data_rows.count())\n",
    "            #df_config_data_rows.display()\n",
    "        # Loop through new Configurations - Skip if Pipeline already created...\n",
    "        for row in df_config_data_rows.collect():\n",
    "            if row['IngestionPipelineId']:\n",
    "                v_return_dict['Skipped'] += 1\n",
    "                v_skipped_message = f\"Warning: SKipping - PipelineId ({row['IngestionPipelineId']}) is filled in for this implementation: Pipeline={row['IngestionPipelineName']}, Server={row['SourceServerName1']}, Database={row['SourceDatabaseName1']}\"\n",
    "                v_skipped_results.append(v_skipped_message)\n",
    "                print(v_skipped_message)\n",
    "            else:\n",
    "                # Get row as Dictionary and Validate Attributes\n",
    "                v_config_row_dict = row.asDict()\n",
    "                v_validation_message = self.validate_config_row(v_config_row_dict)\n",
    "                # If Validate Successfully, add additional Attributes to row dictionary and save in array to process\n",
    "                if v_validation_message is None:\n",
    "                    v_config_row_dict['Status'] = 'Pending'\n",
    "                    v_config_row_dict['DestinationCatalog'] = v_unity_catalog\n",
    "                    v_config_row_dict['process_config'] = process_config\n",
    "                    v_config_row_dict['params_dict'] = self.params_dict\n",
    "\n",
    "                    v_config_row_dict['ProcessIdentifier'] = f\"Pipeline={v_config_row_dict['IngestionPipelineName']} Server={v_config_row_dict['SourceServerName1']} Database={v_config_row_dict['SourceDatabaseName1']}\"\n",
    "\n",
    "                    v_config_row_array.append(v_config_row_dict)\n",
    "\n",
    "                    print(f\"Implementation will proceed for {row['IngestionPipelineName']}, Server={row['SourceServerName1']}, Database={row['SourceDatabaseName1']}\")\n",
    "                else:\n",
    "                    # Collect Validation Failure Details and save in array to display at process end.\n",
    "                    v_return_dict['ValidationFailed'] += 1\n",
    "                    v_validation_message = f\"Implementation will NOT proceed for {row['IngestionPipelineName']}, Server={row['SourceServerName1']}, Database={row['SourceDatabaseName1']}: {v_validation_message}\"\n",
    "                    v_validation_errors.append(v_validation_message)\n",
    "                    print(v_validation_message)\n",
    "\n",
    "        if len(v_config_row_array) == 0:\n",
    "            v_return_dict['ValidationFailures'] = v_validation_errors\n",
    "            v_return_dict['SkippedResults'] = v_skipped_results\n",
    "        else:\n",
    "            # Launch New Pipelines to Create based on Configuration in parallel...\n",
    "            # Collect Thread Results and Errors\n",
    "            with ThreadPoolExecutor(max_workers=self.get_process_max_workers()) as executor:\n",
    "                futures = {executor.submit(self.process_config_row, obj): obj for obj in v_config_row_array}\n",
    "                for future in as_completed(futures):\n",
    "                    obj = futures[future]\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        v_pipeline_id = result['pipeline_id'] if result and 'pipeline_id' in result else None\n",
    "                        v_thread_results.append(str(obj['ProcessIdentifier']) + ' \\nMessage=Ingestion Pipeline Successfully Created - ID: ' + str(v_pipeline_id))\n",
    "                        #print(f\"Thread Successes: \", v_thread_results)\n",
    "                    except Exception as e:\n",
    "                        v_thread_errors.append(str(obj['ProcessIdentifier']) + ' \\nMessage=' + str(e).replace('\\n','')[0:self.get_traceback_length()] + ' \\nTraceback=' + traceback.format_exc().replace('\\n','')[0:self.get_traceback_length()] )\n",
    "                        #print(f\"Thread Errors: \", v_thread_errors)\n",
    "        # Collect Run Statistics\n",
    "        v_return_dict['Success'] = len(v_thread_results)\n",
    "        v_return_dict['ExecutionFailed'] = len(v_thread_errors)\n",
    "        v_return_dict['ExecutionFailures'] = v_thread_errors\n",
    "        v_return_dict['ValidationFailures'] = v_validation_errors\n",
    "        v_return_dict['SuccessResults'] = v_thread_results\n",
    "        v_return_dict['SkippedResults'] = v_skipped_results\n",
    "        # Display Summary\n",
    "        print(\"\\n\\nSummary:\\n\")\n",
    "        pprint.pprint(v_return_dict, indent=4, compact=True, sort_dicts=False, width=10000)\n",
    "        # Raise Exception if any errors were encountered or no Configurations to Process\n",
    "        if len(v_thread_errors) + len(v_validation_errors) > 0:\n",
    "            raise Exception(f\"{len(v_thread_errors)} Execution errors and {len(v_validation_errors)} Validation Errors were encountered during processing. See exception for details.\")\n",
    "        elif len(v_config_row_array) == 0:\n",
    "            raise Exception(f\"Unable to find valid implementation configuration(s) for {self.params_dict}\")\n",
    "        \n",
    "        return v_return_dict\n",
    "\n",
    "    def validate_config_row(self, p_config_row_dict):\n",
    "        validation_message = ''\n",
    "        if p_config_row_dict['SourceServerName1'] is None:\n",
    "            validation_message += 'SourceServerName1 is required\\n'\n",
    "        if p_config_row_dict['SourceDatabaseName1'] is None:\n",
    "            validation_message += 'SourceDatabaseName1 is required\\n'\n",
    "        if p_config_row_dict['IngestionPipelineName'] is None:\n",
    "            validation_message += 'IngestionPipelineName is required\\n'\n",
    "        if p_config_row_dict['DestinationSchema'] is None:\n",
    "            validation_message += 'DestinationSchema is required\\n'\n",
    "        if p_config_row_dict['GatewayPipelineId'] is None:\n",
    "            validation_message += 'GatewayPipelienID is required\\n'\n",
    "        if p_config_row_dict['TableList'] is None:\n",
    "            validation_message += 'TableList is required\\n'\n",
    "        else:\n",
    "            try:\n",
    "                v__json = json.loads(p_config_row_dict['TableList'])\n",
    "            except:\n",
    "                validation_message += 'TableList is not valid JSON\\n'\n",
    "        \n",
    "        return validation_message if validation_message != '' else None\n",
    "\n",
    "    def build_pipeline_json_dict(self, p_config_row_dict):\n",
    "        \n",
    "        return {\n",
    "                \"name\": p_config_row_dict['IngestionPipelineName'],\n",
    "                \"ingestion_definition\": {\n",
    "                                            \"ingestion_gateway_id\": p_config_row_dict['GatewayPipelineId'],\n",
    "                                            \"source_type\": \"SQLSERVER\",\n",
    "                                            \"objects\": [\n",
    "                                                {\n",
    "                                                    \"table\": {\n",
    "                                                        \"source_catalog\": p_config_row_dict['SourceDatabaseName1'],\n",
    "                                                        \"source_schema\": table[\"SourceSchema\"],\n",
    "                                                        \"source_table\": table[\"SourceTable\"],\n",
    "                                                        \"destination_catalog\": p_config_row_dict['DestinationCatalog'],\n",
    "                                                        \"destination_schema\": p_config_row_dict['DestinationSchema'],\n",
    "                                                        \"destination_table\": table[\"DestinationTable\"]\n",
    "                                                    }\n",
    "                                                }\n",
    "                                                for table in json.loads(p_config_row_dict['TableList'])[\"data\"]\n",
    "                                            ]\n",
    "                                        }\n",
    "                }\n",
    "\n",
    "    def process_config_row(self, p_config_row_dict):\n",
    "\n",
    "        print(\"Processing: \", p_config_row_dict['ProcessIdentifier'])\n",
    "        reponse_dict = {}\n",
    "        \n",
    "        try:\n",
    "            # This function can be called outside of the main process function, so we need to make sure we have the process_config object instantiated\n",
    "            if 'process_config' not in p_config_row_dict:\n",
    "                p_config_row_dict['process_config'] = process_config = ProcessConfigData(self.params_dict['p_environment'])\n",
    "\n",
    "            v_api_json_data_dict = self.build_pipeline_json_dict(p_config_row_dict)\n",
    "            #print('v_api_json_data_dict:',v_api_json_data_dict)\n",
    "        \n",
    "            # Ensure Gateway is running before Pipeline Creation\n",
    "            self.start_gateway_pipeline(p_config_row_dict)\n",
    "\n",
    "            v_lakeflow_api = LakeflowIngestionAPI()\n",
    "            reponse_dict = v_lakeflow_api.create_ingestion_pipeline(v_api_json_data_dict)\n",
    "\n",
    "            if reponse_dict['status'] == 'ok':\n",
    "                p_config_row_dict['IngestionPipelineId'] = reponse_dict['pipeline_id']\n",
    "                self.process_config_update_pipeline_id(p_config_row_dict)\n",
    "\n",
    "                # Set Permissions if defined\n",
    "                gb_vars_obj = GlobalVars(self.params_dict['p_environment'])\n",
    "                v_permissions_list = gb_vars_obj.get_pipeline_permissions_list()\n",
    "                if v_permissions_list is not None and len(v_permissions_list) > 0:\n",
    "                    v_lakeflow_api.update_pipeline_permissions(p_config_row_dict['IngestionPipelineId'], v_permissions_list)\n",
    "                \n",
    "                # Initial Start of Ingestion Pipeline\n",
    "                self.start_initial_ingestion_pipeline(p_config_row_dict, v_lakeflow_api)\n",
    "\n",
    "                # Potentially Shut Down Gateway Pipeline - TBD based on Observation\n",
    "                # self.stop_gateway_pipeline(p_config_row_dict)\n",
    "            else:\n",
    "                raise Exception (f\"Error: Create Ingestion Pipeline Failed for {p_config_row_dict['ProcessIdentifier']}: str({reponse_dict})\")\n",
    "            #print(\"Response:\", reponse_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {p_config_row_dict['ProcessIdentifier']} {e}\")\n",
    "            raise Exception(f\"Error: {p_config_row_dict['ProcessIdentifier']} {e}\")\n",
    "        return reponse_dict\n",
    "    \n",
    "    def start_initial_ingestion_pipeline(self, p_config_row_dict, api_obj):\n",
    "\n",
    "        # Set Pipeline Status on Process Table\n",
    "        self.set_pipeline_process_status(p_config_row_dict, 'P')\n",
    "\n",
    "        # Snapshots Start Time in UTC to watch for Ingestion Pipeline\n",
    "        v_snapshot_start_time = self.get_snapshot_startime()\n",
    "        print(f\"v_snapshot_start_time: {v_snapshot_start_time} Id={p_config_row_dict['GatewayPipelineId']}\")\n",
    "\n",
    "        # Start Initial Ingestion Pipeline\n",
    "        api_obj.start_pipeline(p_config_row_dict['IngestionPipelineId'])\n",
    "\n",
    "        # Wait for All Snaphots to complete for current Pipeline\n",
    "        self.wait_for_snapshots_complete(p_config_row_dict, p_config_row_dict['TableCount'], v_snapshot_start_time, api_obj)\n",
    "        \n",
    "        # Start Pipeline to process Snapshots\n",
    "        api_obj.start_pipeline(p_config_row_dict['IngestionPipelineId'])\n",
    "\n",
    "        v_retry_max = 20\n",
    "        v_retry_count = 0\n",
    "        v_ingestion_last_update_state = None\n",
    "\n",
    "        # Wait for Ingestion Pipeline to be IDLE\n",
    "        v_pipeline_dict = api_obj.get_pipeline(p_config_row_dict['IngestionPipelineId'])\n",
    "        if v_pipeline_dict['status'] == 'ok':\n",
    "            v_state = v_pipeline_dict['state']\n",
    "            v_ingestion_last_update_state = v_pipeline_dict['latest_update_state']\n",
    "            v_retry_count = 0\n",
    "            while v_state != 'IDLE' and v_retry_count < v_retry_max:\n",
    "                time.sleep(60)\n",
    "                v_pipeline_dict = api_obj.get_pipeline(p_config_row_dict['IngestionPipelineId'])\n",
    "                v_state = v_pipeline_dict['state']\n",
    "                v_ingestion_last_update_state = v_pipeline_dict['latest_update_state']\n",
    "                v_retry_count += 1\n",
    "        \n",
    "        if v_ingestion_last_update_state != 'COMPLETED':\n",
    "            # Set Pipeline Failed Status on Process Table\n",
    "            self.set_pipeline_process_status(p_config_row_dict, 'F')\n",
    "            # Raise Exception\n",
    "            raise Exception(f\"Error: Ingestion Pipeline Initial Run Failed for {p_config_row_dict['ProcessIdentifier']}: {v_ingestion_last_update_state} - {v_pipeline_dict}\")\n",
    "        else:\n",
    "            # Set Pipeline Success Status on Process Table\n",
    "            self.set_pipeline_process_status(p_config_row_dict, 'C')\n",
    "        \n",
    "        return v_pipeline_dict\n",
    "\n",
    "    def process_config_update_pipeline_id(self, p_config_row_dict):\n",
    "        v_stored_procedure_params_dict = { \n",
    "                                            'PipelineType': 'Ingestion',\n",
    "                                            'InternalProductId': p_config_row_dict['InternalProductId'],\n",
    "                                            'SourceServerName1': p_config_row_dict['SourceServerName1'],\n",
    "                                            'SourceDatabaseName1': p_config_row_dict['SourceDatabaseName1'],\n",
    "                                            'SourceConfigTable': p_config_row_dict['SourceConfigTable'],\n",
    "                                            'PipelineId': p_config_row_dict['IngestionPipelineId'],\n",
    "                                        }\n",
    "\n",
    "        p_config_row_dict['process_config'].set_pipeline_id(v_stored_procedure_params_dict)\n",
    "\n",
    "    def get_ingestion_api(self):\n",
    "        return LakeflowIngestionAPI()\n",
    "    \n",
    "    def set_pipeline_process_status(self, p_config_row_dict, p_status):\n",
    "        v_stored_procedure_params_dict = {  'PipelineName': p_config_row_dict['IngestionPipelineName'], \\\n",
    "                                            'InternalProductId' : p_config_row_dict['InternalProductId'], \\\n",
    "                                            'DataSourceId' : p_config_row_dict['DataSourceId'], \\\n",
    "                                            'InternalClientId' : p_config_row_dict['InternalClientId'], \\\n",
    "                                            'InternalFacilityId' : p_config_row_dict['InternalFacilityId'], \\\n",
    "                                            'StepType' : 'Extract', \\\n",
    "                                            'Status': p_status\n",
    "                                        }\n",
    "        p_config_row_dict['process_config'].set_pipeline_process_status(v_stored_procedure_params_dict)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IngestionManagementHandler",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
