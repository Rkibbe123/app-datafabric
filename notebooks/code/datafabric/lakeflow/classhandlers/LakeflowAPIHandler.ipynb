{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec843d6d-497b-47cc-a5fc-48caf3c02f48",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Python Modules"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import catalog, jobs, pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e152ff8-5a71-40e0-8daa-d4c5e99f2987",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LakeflowAPI"
    }
   },
   "outputs": [],
   "source": [
    "class LakeflowAPI:\n",
    "    def __init__(self):\n",
    "        notebook_context  = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "        self.api_token = notebook_context.apiToken().get()\n",
    "        self.databricks_url = notebook_context.apiUrl().get()\n",
    "        self.api_base_headers_dict = {'Authorization': f'Bearer {self.api_token}', \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def get_workspace_api_url(self):\n",
    "        return self.databricks_url\n",
    "    \n",
    "    def build_api_header_dict(self, p_headers_dict=None):\n",
    "        return self.api_base_headers_dict | (p_headers_dict or {})\n",
    "\n",
    "    def get_pipeline_api_url(self):\n",
    "        workspace_api_url = self.get_workspace_api_url()\n",
    "        return workspace_api_url + \"/api/2.0/pipelines\"\n",
    "\n",
    "    def get_catalog_api_url(self):\n",
    "        workspace_api_url = self.get_workspace_api_url()\n",
    "        return workspace_api_url + \"/api/2.0/unity-catalog/catalogs\"\n",
    "\n",
    "    def create_pipeline(self, p_pipeline_json, p_headers_dict=None):\n",
    "        print(\"\\nCreating pipeline...\\n\")\n",
    "        return_dict = {'status': 'error'}\n",
    "                \n",
    "        pipeline_api_url = self.get_pipeline_api_url()\n",
    "        headers_dict = self.build_api_header_dict(p_headers_dict)\n",
    "\n",
    "        return_dict['pipeline_api_url'] = pipeline_api_url\n",
    "        return_dict['headers_dict'] = headers_dict\n",
    "        return_dict['pipeline_json'] = p_pipeline_json\n",
    "        \n",
    "        response = requests.post(pipeline_api_url, headers=headers_dict, json=p_pipeline_json)\n",
    "        return_dict['response'] = response.json()\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return_dict['status'] = 'ok'\n",
    "            return_dict['pipeline_id'] = return_dict['response'][\"pipeline_id\"]\n",
    "\n",
    "        #print(\"\\n\",\"return_dict:\",return_dict,\"\\n\")\n",
    "\n",
    "        return return_dict\n",
    "    \n",
    "    def update_pipeline(self, p_pipeline_id, p_pipeline_json, p_headers_dict=None):\n",
    "        print(f\"\\nUpdating pipeline...{p_pipeline_id}\\n\")\n",
    "        return_dict = {'status': 'error'}\n",
    "\n",
    "        pipeline_api_url = self.get_pipeline_api_url() + \"/\" + str(p_pipeline_id)\n",
    "        headers_dict = self.build_api_header_dict(p_headers_dict)\n",
    "\n",
    "        return_dict['pipeline_api_url'] = pipeline_api_url\n",
    "        return_dict['headers_dict'] = headers_dict\n",
    "        return_dict['pipeline_json'] = p_pipeline_json\n",
    "        \n",
    "        response = requests.put(pipeline_api_url, headers=headers_dict, json=p_pipeline_json)\n",
    "        return_dict['response'] = response.json()\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return_dict['status'] = 'ok'\n",
    "            return_dict['pipeline_id'] = p_pipeline_id\n",
    "\n",
    "        #print(\"\\n\",f\"{p_pipeline_id} - return_dict:\",return_dict,\"\\n\")\n",
    "\n",
    "        return return_dict\n",
    "    \n",
    "    def start_pipeline(self, p_pipeline_id, p_full_refresh=False, p_refresh_table_list=None, p_full_refresh_table_list=None, p_cause=\"No Reason passed\",  p_headers_dict=None):\n",
    "\n",
    "        print(f\"\\nStarting pipeline...{p_pipeline_id}\\n\")\n",
    "        return_dict = {'status': 'error'}\n",
    "        \n",
    "        v_pipeline_json = {} \n",
    "\n",
    "        v_pipeline_json['full_refresh'] = p_full_refresh\n",
    "        v_pipeline_json['cause'] = p_cause\n",
    "\n",
    "        if not p_full_refresh:\n",
    "            if p_refresh_table_list and len(p_refresh_table_list) > 0:\n",
    "                v_pipeline_json['refresh_selection'] = p_refresh_table_list\n",
    "            if p_full_refresh_table_list and len(p_full_refresh_table_list) > 0:\n",
    "                v_pipeline_json['full_refresh_selection'] = p_full_refresh_table_list\n",
    "\n",
    "        pipeline_api_url = self.get_pipeline_api_url() + \"/\" + str(p_pipeline_id) + \"/updates\"\n",
    "        headers_dict = self.build_api_header_dict(p_headers_dict)\n",
    "\n",
    "        return_dict['pipeline_api_url'] = pipeline_api_url\n",
    "        return_dict['headers_dict'] = headers_dict\n",
    "        return_dict['pipeline_json'] = v_pipeline_json\n",
    "\n",
    "        response = requests.post(pipeline_api_url, headers=headers_dict, json=v_pipeline_json)\n",
    "\n",
    "        return_dict['response'] = response.json()\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return_dict['status'] = 'ok'\n",
    "            return_dict['pipeline_update_id'] = return_dict['response'][\"update_id\"]\n",
    "\n",
    "        return return_dict\n",
    "\n",
    "    def stop_pipeline(self, p_pipeline_id, p_headers_dict=None):\n",
    "        print(f\"\\nStopping pipeline...{p_pipeline_id}\\n\")\n",
    "\n",
    "        return_dict = {'status': 'error'}\n",
    "        v_pipeline_json = {}\n",
    "\n",
    "        pipeline_api_url = self.get_pipeline_api_url() + \"/\" + str(p_pipeline_id) + \"/stop\"\n",
    "        headers_dict = self.build_api_header_dict(p_headers_dict)\n",
    "\n",
    "        return_dict['pipeline_api_url'] = pipeline_api_url\n",
    "        return_dict['headers_dict'] = headers_dict\n",
    "        return_dict['pipeline_json'] = v_pipeline_json\n",
    "\n",
    "        response = requests.post(pipeline_api_url, headers=headers_dict, json=v_pipeline_json)\n",
    "        return_dict['response'] = response.json()\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return_dict['status'] = 'ok'\n",
    "\n",
    "        return return_dict\n",
    "    \n",
    "    def get_pipeline(self, p_pipeline_id, p_headers_dict=None):\n",
    "        print(f\"\\nGetting pipeline definition...{p_pipeline_id}\\n\")\n",
    "        return_dict = {'status': 'error'}\n",
    "        \n",
    "        pipeline_api_url = self.get_pipeline_api_url() + \"/\" + str(p_pipeline_id)\n",
    "        headers_dict = self.build_api_header_dict(p_headers_dict)\n",
    "\n",
    "        return_dict['pipeline_api_url'] = pipeline_api_url\n",
    "        return_dict['headers_dict'] = headers_dict\n",
    "\n",
    "        response = requests.get(pipeline_api_url, headers=headers_dict)\n",
    "        return_dict['response'] = response.json()\n",
    "        return_dict['pipeline_id'] = p_pipeline_id\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return_dict['status'] = 'ok'\n",
    "            return_dict['name'] = return_dict['response']['name']\n",
    "            return_dict['state'] = return_dict['response']['state']\n",
    "            \n",
    "            return_dict['latest_update_state'] = return_dict['response']['latest_updates'][0]['state'] \\\n",
    "                        if 'latest_updates' in return_dict['response'] else return_dict['state']\n",
    "            \n",
    "        return return_dict\n",
    "    \n",
    "    def get_pipeline_updates(self, p_pipeline_id, p_until_update_id=None, p_page_token=None, p_max_results=None, p_headers_dict=None):\n",
    "        print(f\"\\nGetting pipeline updates...{p_pipeline_id} - {p_until_update_id}\\n\")\n",
    "        return_dict = {'status': 'error'}\n",
    "        \n",
    "        query_params_dict = {'until_update_id' : p_until_update_id, 'page_token' : p_page_token, 'max_results' : p_max_results}\n",
    "\n",
    "        pipeline_api_url = self.get_pipeline_api_url() + \"/\" + str(p_pipeline_id) + \"/updates\"\n",
    "        \n",
    "        query_string = self.build_query_string(query_params_dict)\n",
    "        if query_string:\n",
    "            pipeline_api_url += \"?\" + query_string\n",
    "\n",
    "        headers_dict = self.build_api_header_dict(p_headers_dict)\n",
    "\n",
    "        return_dict['pipeline_api_url'] = pipeline_api_url\n",
    "        return_dict['headers_dict'] = headers_dict\n",
    "\n",
    "        response = requests.get(pipeline_api_url, headers=headers_dict)\n",
    "        return_dict['response'] = response.json()\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return_dict['status'] = 'ok'\n",
    "            return_dict['pipeline_id'] = p_pipeline_id\n",
    "            \n",
    "        return return_dict\n",
    "    \n",
    "    def get_pipeline_events(self, p_pipeline_id, p_filter=None, p_page_token=None, p_max_results=250, p_order_by=None, p_headers_dict=None):\n",
    "        #print(f\"\\nGetting pipeline events...{p_pipeline_id} - {p_filter}\\n\")\n",
    "        return_dict = {'status': 'error'}\n",
    "        \n",
    "        query_params_dict = {'filter' : p_filter, 'page_token' : p_page_token, 'max_results' : p_max_results, 'order_by' : p_order_by}\n",
    "\n",
    "        pipeline_api_url = self.get_pipeline_api_url() + \"/\" + str(p_pipeline_id) + \"/events\"\n",
    "        \n",
    "        query_string = self.build_query_string(query_params_dict)\n",
    "        if query_string:\n",
    "            pipeline_api_url += \"?\" + query_string\n",
    "\n",
    "        headers_dict = self.build_api_header_dict(p_headers_dict)\n",
    "\n",
    "        return_dict['pipeline_api_url'] = pipeline_api_url\n",
    "        return_dict['headers_dict'] = headers_dict\n",
    "\n",
    "        response = requests.get(pipeline_api_url, headers=headers_dict)\n",
    "        return_dict['response'] = response.json()\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return_dict['status'] = 'ok'\n",
    "            return_dict['pipeline_id'] = p_pipeline_id\n",
    "            \n",
    "        return return_dict\n",
    "    \n",
    "    def build_query_string (self, p_dict):\n",
    "        query_string = \"&\".join([f\"{k}={v}\" for k,v in p_dict.items() if v is not None])\n",
    "        return query_string if len(query_string) > 0 else None\n",
    "    \n",
    "    def get_pipeline_permissions(self, p_pipeline_id, p_headers_dict=None):\n",
    "        print(f\"\\nGet pipeline permissions...{p_pipeline_id}\\n\")\n",
    "        return_dict = {'status': 'error'}\n",
    "        \n",
    "        pipeline_api_url = self.get_workspace_api_url() + \"/api/2.0/permissions/pipelines/\" + str(p_pipeline_id)\n",
    "        headers_dict = self.build_api_header_dict(p_headers_dict)\n",
    "\n",
    "        return_dict['pipeline_api_url'] = pipeline_api_url\n",
    "        return_dict['headers_dict'] = headers_dict\n",
    "\n",
    "        response = requests.get(pipeline_api_url, headers=headers_dict)\n",
    "        return_dict['response'] = response.json()\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return_dict['status'] = 'ok'\n",
    "        \n",
    "        return return_dict\n",
    "\n",
    "    def update_pipeline_permissions(self, p_pipeline_id, p_permissions_list, p_headers_dict=None):\n",
    "        # Add to existing permissions - do not require IS_OWNER to be set\n",
    "        print(f\"\\nUpdating pipeline permissions...{p_pipeline_id}\\n\")\n",
    "        return_dict = {'status': 'error'}\n",
    "\n",
    "        v_pipeline_json_dict = {}\n",
    "        v_pipeline_json_dict['access_control_list'] = p_permissions_list\n",
    "        v_pipeline_json = json.dumps(v_pipeline_json_dict, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        pipeline_api_url = self.get_workspace_api_url() + \"/api/2.0/permissions/pipelines/\" + str(p_pipeline_id)\n",
    "        headers_dict = self.build_api_header_dict(p_headers_dict)\n",
    "\n",
    "        return_dict['pipeline_api_url'] = pipeline_api_url\n",
    "        return_dict['headers_dict'] = headers_dict\n",
    "\n",
    "        response = requests.patch(pipeline_api_url, headers=headers_dict, json=v_pipeline_json_dict)\n",
    "        return_dict['response'] = response.json()\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return_dict['status'] = 'ok'\n",
    "        \n",
    "        return return_dict\n",
    "    \n",
    "    def set_pipeline_permissions(self, p_pipeline_id, p_permissions_list, p_headers_dict=None):\n",
    "        # Requires IS_OWNER to be set\n",
    "        print(f\"\\nUpdating pipeline permissions...{p_pipeline_id}\\n\")\n",
    "        return_dict = {'status': 'error'}\n",
    "\n",
    "        v_pipeline_json_dict = {}\n",
    "        v_pipeline_json_dict['access_control_list'] = p_permissions_list\n",
    "        v_pipeline_json = json.dumps(v_pipeline_json_dict, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        pipeline_api_url = self.get_workspace_api_url() + \"/api/2.0/permissions/pipelines/\" + str(p_pipeline_id)\n",
    "        headers_dict = self.build_api_header_dict(p_headers_dict)\n",
    "\n",
    "        return_dict['pipeline_api_url'] = pipeline_api_url\n",
    "        return_dict['headers_dict'] = headers_dict\n",
    "\n",
    "        response = requests.put(pipeline_api_url, headers=headers_dict, json=v_pipeline_json_dict)\n",
    "        return_dict['response'] = response.json()\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return_dict['status'] = 'ok'\n",
    "        \n",
    "        return return_dict\n",
    "\n",
    "    def delete_pipeline(self, p_pipeline_id, p_headers_dict=None):\n",
    "        print(f\"\\nDeleting pipeline...{p_pipeline_id}\\n\")\n",
    "        return_dict = {'status': 'error'}\n",
    "        \n",
    "        pipeline_api_url = self.get_pipeline_api_url() + \"/\" + str(p_pipeline_id)\n",
    "        headers_dict = self.build_api_header_dict(p_headers_dict)\n",
    "\n",
    "        return_dict['pipeline_api_url'] = pipeline_api_url\n",
    "        return_dict['headers_dict'] = headers_dict\n",
    "\n",
    "        response = requests.delete(pipeline_api_url, headers=headers_dict)\n",
    "        return_dict['response'] = response.json()\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return_dict['status'] = 'ok'\n",
    "\n",
    "        return return_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e5c11b9-503a-419e-b944-044990aee923",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LakeflowAPIExtension"
    }
   },
   "outputs": [],
   "source": [
    "class LakeflowAPIExtension():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.workspace_client = WorkspaceClient()\n",
    "\n",
    "    def wait_for_gateway_running(self, gateway_pipeline_id: str, max_wait_minutes: int = 30) -> bool:\n",
    "        \"\"\"\n",
    "        Wait for gateway pipeline to reach RUNNING state.\n",
    "        This is required before we can query event logs.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if gateway is running, False if timeout or failure\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"WAITING FOR GATEWAY TO REACH RUNNING STATE\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Gateway ID: {gateway_pipeline_id}\")\n",
    "        print(f\"Max wait: {max_wait_minutes} minutes\")\n",
    "        print(\"Note: Event logs cannot be queried until gateway is RUNNING\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        max_wait_seconds = max_wait_minutes * 60\n",
    "        check_interval = 15  # Check every 15 seconds\n",
    "        \n",
    "        try:\n",
    "            while (time.time() - start_time) < max_wait_seconds:\n",
    "                # Get gateway pipeline status\n",
    "                pipeline = self.workspace_client.pipelines.get(gateway_pipeline_id)\n",
    "                pipeline_state = pipeline.state.value if pipeline.state else 'UNKNOWN'\n",
    "                \n",
    "                elapsed = int(time.time() - start_time)\n",
    "                print(f\"   [{elapsed}s] Gateway ({gateway_pipeline_id}) state: {pipeline_state}\")\n",
    "                \n",
    "                # Check if gateway is running\n",
    "                if pipeline_state == 'RUNNING':\n",
    "                    # Also check latest update state if available\n",
    "                    if pipeline.latest_updates:\n",
    "                        latest_update = pipeline.latest_updates[0]\n",
    "                        update_state = latest_update.state.value if latest_update.state else 'UNKNOWN'\n",
    "                        print(f\" Latest update state ({gateway_pipeline_id}): {update_state}\")\n",
    "                        \n",
    "                        if update_state in ['COMPLETED', 'RUNNING']:\n",
    "                            print(f\"\\n✅ SUCCESS: Gateway ({gateway_pipeline_id}) is RUNNING with update state {update_state}!\")\n",
    "                            return True\n",
    "                        else:\n",
    "                            print(f\"   Waiting for update to complete (current: {update_state})...\")\n",
    "                    else:\n",
    "                        # No updates yet but pipeline is RUNNING\n",
    "                        print(f\"\\n✅ SUCCESS: Gateway ({gateway_pipeline_id}) is RUNNING (no updates yet)!\")\n",
    "                        return True\n",
    "                \n",
    "                # Check for failure states\n",
    "                if pipeline_state in ['FAILED', 'DELETED']:\n",
    "                    print(f\"\\n❌ ERROR: Gateway ({gateway_pipeline_id}) is in {pipeline_state} state\")\n",
    "                    return False\n",
    "                \n",
    "                # Wait before next check\n",
    "                time.sleep(check_interval)\n",
    "            \n",
    "            # Timeout\n",
    "            print(f\"\\n⏰ TIMEOUT: Gateway ({gateway_pipeline_id}) did not reach RUNNING state within {max_wait_minutes} minutes\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ ERROR: Failed to check gateway ({gateway_pipeline_id}) status: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def wait_for_snapshots_complete(\n",
    "        self,\n",
    "        gateway_pipeline_id: str,\n",
    "        ingestion_id: str,\n",
    "        expected_table_count: int,\n",
    "        snapshot_start_time: datetime = '2000-01-01T00:00:00.000Z', #datetime.now(),\n",
    "        start_ingestion_flag: bool = False,\n",
    "        max_wait_minutes: int = 120\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Wait for all table snapshots to complete using event log.\n",
    "        \n",
    "        IMPORTANT: Gateway must be in RUNNING state before this method is called.\n",
    "        This method will first ensure gateway is running, then monitor event logs.\n",
    "        \n",
    "        Query checks: SELECT COUNT(*) FROM event_log(gateway_id)\n",
    "                      WHERE event_type = 'flow_progress'\n",
    "                      AND CAST(PARSE_JSON(message):eventType AS STRING) = 'SNAPSHOT_COMPLETED'\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if all snapshots completed, False if timeout or failure\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"WAITING FOR GATEWAY SNAPSHOTS TO COMPLETE\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Gateway ID: {gateway_pipeline_id}\")\n",
    "        print(f\"Expected tables: {expected_table_count}\")\n",
    "        print(f\"Max wait: {max_wait_minutes} minutes\")\n",
    "        \n",
    "        # STEP 1: Ensure gateway is in RUNNING state before querying event logs\n",
    "        print(f\"\\n[STEP 1] Ensuring gateway is in RUNNING state.. IDs {gateway_pipeline_id}/{ingestion_id}\")\n",
    "        gateway_running = self.wait_for_gateway_running(gateway_pipeline_id, max_wait_minutes=20)\n",
    "        \n",
    "        if not gateway_running:\n",
    "            print(\"\\n❌ ERROR: Gateway did not reach RUNNING state\")\n",
    "            print(\"   Cannot query event logs until gateway is RUNNING\")\n",
    "            return False\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"TRIGGERING INITIAL INGESTION (TO START GATEWAY SNAPSHOTS)\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Note: The first ingestion run triggers the gateway to start taking snapshots\")\n",
    "        print(\"      We'll wait for snapshots to complete, then run ingestion again\")\n",
    "        print(\"      Pipeline already has table configuration - no need to specify tables\")\n",
    "\n",
    "        if start_ingestion_flag:\n",
    "            api_obj = LakeflowAPI()\n",
    "            response_dict = api_obj.start_pipeline(ingestion_id)\n",
    "            initial_update_id = response_dict['pipeline_update_id'] if response_dict['status'] == 'ok' else 'not_found'\n",
    "\n",
    "            print(f\"\\n✓ Initial ingestion triggered - Update ID: {initial_update_id}\")\n",
    "            #print(f\"  This will cause the gateway to begin snapshot process for all {len(tables)} configured tables\")\n",
    "\n",
    "        print(f\"\\n[STEP 2] Gateway is RUNNING - monitoring snapshot progress via event logs - IDs {gateway_pipeline_id}/{ingestion_id}...\")\n",
    "        \n",
    "        # SQL query to count completed snapshots\n",
    "        snapshot_query = f\"\"\"\n",
    "        SELECT COUNT(*) as completed_count\n",
    "        FROM event_log('{gateway_pipeline_id}')\n",
    "        WHERE event_type = 'flow_progress'\n",
    "          AND timestamp >= '{snapshot_start_time}'\n",
    "          AND CAST(PARSE_JSON(message):eventType AS STRING) = 'SNAPSHOT_COMPLETED'\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        max_wait_seconds = max_wait_minutes * 60\n",
    "        check_interval = 15  # Check every 15 seconds\n",
    "        elapsed_time = 0\n",
    "        last_count = 0\n",
    "        error_flag = False\n",
    "        \n",
    "        try:\n",
    "            while elapsed_time < max_wait_seconds:\n",
    "                \n",
    "                try:\n",
    "                    result = spark.sql(snapshot_query).collect()   \n",
    "                    \n",
    "                    if result:\n",
    "                        completed_count = result[0]['completed_count']\n",
    "                        progress_pct = (completed_count / expected_table_count * 100) if expected_table_count > 0 else 0\n",
    "\n",
    "                        # Print progress update\n",
    "                        elapsed = int(time.time() - start_time)\n",
    "                        if completed_count != last_count or elapsed % 30 == 0:\n",
    "                            print(f\"\\n⏱  [{elapsed}s] Snapshot Progress: {progress_pct:.1f}%\")\n",
    "                            print(f\"   Completed: {completed_count}/{expected_table_count} tables\")\n",
    "                            \n",
    "                            # Progress bar\n",
    "                            bar_length = 50\n",
    "                            filled = int(bar_length * completed_count / expected_table_count) if expected_table_count > 0 else 0\n",
    "                            bar = '█' * filled + '░' * (bar_length - filled)\n",
    "                            print(f\"   [{bar}]\")\n",
    "                            \n",
    "                            last_count = completed_count\n",
    "                        \n",
    "                        # Check if all completed\n",
    "                        if completed_count >= expected_table_count:\n",
    "                            total_time = int(time.time() - start_time)\n",
    "                            print(f\"\\n{'=' * 80}\")\n",
    "                            print(f\"✅ SUCCESS: All {expected_table_count} table snapshots completed!\")\n",
    "                            print(f\"{'=' * 80}\")\n",
    "                            print(f\"   Total time: {int(total_time / 60)} min {total_time % 60} sec\")\n",
    "                            print(f\"   Gateway SqlEvent ({gateway_pipeline_id}) is ready!\")\n",
    "                            return True\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    elapsed = int(time.time() - start_time)\n",
    "                    print(f\"\\n   [{elapsed}s] ⚠ IDs {gateway_pipeline_id}/{ingestion_id} - Could not query event log: {e}\")\n",
    "                    error_flag = True\n",
    "                    break\n",
    "                    #print(f\"      Retrying...\")\n",
    "                \n",
    "                # Wait before next check\n",
    "                time.sleep(check_interval)\n",
    "                elapsed_time = int(time.time() - start_time)\n",
    "            \n",
    "            if not error_flag:\n",
    "                # Timeout\n",
    "                print(f\"\\n{'=' * 80}\")\n",
    "                print(f\"⏰ TIMEOUT: Snapshots did not complete within {max_wait_minutes} minutes IDs {gateway_pipeline_id}/{ingestion_id}\")\n",
    "                print(f\"{'=' * 80}\")\n",
    "                print(f\"   Completed: {last_count}/{expected_table_count} tables\")\n",
    "                print(f\"   You may need to:\")\n",
    "                print(f\"   1. Increase max_wait_minutes\")\n",
    "                print(f\"   2. Check gateway status in Databricks UI\")\n",
    "                print(f\"   3. Split into smaller pipelines (50+ tables)\")\n",
    "                return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ ERROR: Failed to monitor snapshots: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def wait_for_pipeline_idle(self, pipeline_id: str, max_wait_minutes: int = 30) -> bool:\n",
    "        \"\"\"\n",
    "        Wait for pipeline to reach IDLE state (not running).\n",
    "        \n",
    "        Args:\n",
    "            pipeline_id: Pipeline ID to check\n",
    "            max_wait_minutes: Maximum minutes to wait\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if pipeline is idle, False if timeout or failure\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"CHECKING PIPELINE STATE\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        print(f\"Pipeline ID: {pipeline_id}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        max_wait_seconds = max_wait_minutes * 60\n",
    "        check_interval = 10  # Check every 10 seconds\n",
    "        \n",
    "        try:\n",
    "            while (time.time() - start_time) < max_wait_seconds:\n",
    "                # Get pipeline status\n",
    "                pipeline = self.workspace_client.pipelines.get(pipeline_id)\n",
    "                pipeline_state = pipeline.state.value if pipeline.state else 'UNKNOWN'\n",
    "                \n",
    "                elapsed = int(time.time() - start_time)\n",
    "                \n",
    "                # Check if pipeline is idle\n",
    "                if pipeline_state == 'IDLE':\n",
    "                    print(f\"✓ Pipeline is IDLE - ready for next run\")\n",
    "                    return True\n",
    "                elif pipeline_state == 'RUNNING':\n",
    "                    # Check update state\n",
    "                    if pipeline.latest_updates:\n",
    "                        latest_update = pipeline.latest_updates[0]\n",
    "                        update_state = latest_update.state.value if latest_update.state else 'UNKNOWN'\n",
    "                        print(f\"   [{elapsed}s] Pipeline state: {pipeline_state}, Update state: {update_state}\")\n",
    "                        \n",
    "                        # Check if update is in terminal state\n",
    "                        if update_state in ['COMPLETED', 'FAILED', 'CANCELED']:\n",
    "                            print(f\"   Update {update_state} - waiting for pipeline to become IDLE...\")\n",
    "                        else:\n",
    "                            print(f\"   Pipeline is running (update: {update_state})...\")\n",
    "                    else:\n",
    "                        print(f\"   [{elapsed}s] Pipeline state: {pipeline_state} (no updates)\")\n",
    "                elif pipeline_state in ['FAILED', 'DELETED']:\n",
    "                    print(f\"⚠ WARNING: Pipeline is in {pipeline_state} state\")\n",
    "                    return False\n",
    "                else:\n",
    "                    print(f\"   [{elapsed}s] Pipeline state: {pipeline_state}\")\n",
    "                \n",
    "                # Wait before next check\n",
    "                time.sleep(check_interval)\n",
    "            \n",
    "            # Timeout\n",
    "            print(f\"\\n⏰ TIMEOUT: Pipeline did not become IDLE within {max_wait_minutes} minutes\")\n",
    "            print(f\"   You may continue anyway or wait manually\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠ WARNING: Failed to check pipeline status: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "    def wait_for_snapshots_complete_via_api(self,\n",
    "        gateway_pipeline_id: str,\n",
    "        ingestion_id: str,\n",
    "        expected_table_count: int,\n",
    "        start_timestamp: datetime = '2000-01-01T00:00:00.000Z',\n",
    "        start_ingestion_flag: bool = False,\n",
    "        max_wait_minutes: int = 120\n",
    "        ) -> bool:\n",
    "        \"\"\"\n",
    "        Wait for all snapshots to complete using event log.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if all snapshots completed, False if timeout or failure\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"WAITING FOR GATEWAY SNAPSHOTS TO COMPLETE\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "        print(f\"Waiting for snapshots to complete for ingestion {ingestion_id}...\")\n",
    "        print(f\"Gateway ID: {gateway_pipeline_id}\")\n",
    "        print(f\"Ingestion ID: {ingestion_id}\")\n",
    "        print(f\"Max wait: {max_wait_minutes} minutes\")\n",
    "        print(\"Note: Event logs cannot be queried until gateway is RUNNING\")\n",
    "        print(\"Note: If your pipeline is not in RUNNING state, this method will fail\")\n",
    "        \n",
    "        # STEP 1: Ensure gateway is in RUNNING state before querying event logs\n",
    "        print(f\"\\n[STEP 1] Ensuring gateway is in RUNNING state.. IDs {gateway_pipeline_id}/{ingestion_id}\")\n",
    "        # gateway_running = self.wait_for_gateway_running(gateway_pipeline_id, max_wait_minutes=20)\n",
    "        \n",
    "        # if not gateway_running:\n",
    "        #     print(\"\\n❌ ERROR: Gateway did not reach RUNNING state\")\n",
    "        #     print(\"   Cannot query event logs until gateway is RUNNING\")\n",
    "        #     return False\n",
    "        \n",
    "        api_obj = LakeflowAPI()\n",
    "\n",
    "        start_time = time.time()\n",
    "        max_wait_seconds = max_wait_minutes * 60\n",
    "        check_interval = 15  # Check every 15 seconds\n",
    "        max_zero_count = 30  # Maximum number of consecutive zero counts before timeout\n",
    "        zero_count = 0\n",
    "        elapsed_time = 0\n",
    "        last_count = 0\n",
    "        error_flag = False\n",
    "\n",
    "        last_timestamp = start_timestamp\n",
    "        snapshot_completed_count = 0\n",
    "        snapshot_completed_total = 0\n",
    "\n",
    "        while elapsed_time < max_wait_seconds:\n",
    "\n",
    "            filter = f\"level='INFO' AND timestamp >= '{last_timestamp}'\"\n",
    "            return_events = api_obj.get_pipeline_events(\n",
    "                                                        p_pipeline_id=gateway_pipeline_id,\n",
    "                                                        p_filter=filter,\n",
    "                                                        p_page_token=None,\n",
    "                                                        p_max_results=250,\n",
    "                                                        p_order_by='timestamp asc')\n",
    "            \n",
    "            events_count = len(return_events['response']['events']) if return_events['status'] == 'ok' and 'events' in return_events['response'] else 0\n",
    "            print(f\"Event Response: Status={return_events['status']} Count={events_count} Filter={filter} Id={gateway_pipeline_id} ZeroCount={zero_count} Elapsed={elapsed_time} Max={max_wait_seconds}\")   \n",
    "\n",
    "            if  return_events['status'] != 'ok':\n",
    "                break\n",
    "        \n",
    "            #Check if all snapshots completed\n",
    "            snapshot_completed_count = 0\n",
    "            if events_count > 0:\n",
    "                last_timestamp = return_events['response']['events'][-1]['timestamp']\n",
    "                for event in return_events['response']['events']:\n",
    "                    if event['event_type'] == 'flow_progress':\n",
    "                        event_message = json.loads(event['message'])\n",
    "                        if event_message['eventType'] == 'SNAPSHOT_COMPLETED':\n",
    "                            snapshot_completed_count += 1\n",
    "\n",
    "                snapshot_completed_total += snapshot_completed_count\n",
    "                print(f\"Snapshot Tracker: {snapshot_completed_count} / {snapshot_completed_total}\")\n",
    "\n",
    "                progress_pct = (snapshot_completed_total / expected_table_count * 100) if expected_table_count > 0 else 0\n",
    "\n",
    "                # Print progress update\n",
    "                elapsed = int(time.time() - start_time)\n",
    "                if snapshot_completed_total != last_count or elapsed % 30 == 0:\n",
    "                    print(f\"\\n⏱  [{elapsed}s] Snapshot Progress: {progress_pct:.1f}%\")\n",
    "                    print(f\"   Completed: {snapshot_completed_total}/{expected_table_count} tables\")\n",
    "                    \n",
    "                    # Progress bar\n",
    "                    bar_length = 50\n",
    "                    filled = int(bar_length * snapshot_completed_total / expected_table_count) if expected_table_count > 0 else 0\n",
    "                    bar = '█' * filled + '░' * (bar_length - filled)\n",
    "                    print(f\"   [{bar}]\")\n",
    "                    \n",
    "                    last_count = snapshot_completed_total\n",
    "                \n",
    "                if snapshot_completed_total >= expected_table_count:\n",
    "                    total_time = int(time.time() - start_time)\n",
    "                    print(f\"\\n{'=' * 80}\")\n",
    "                    print(f\"\\n✅ SUCCESS: All {expected_table_count} snapshots completed!\")\n",
    "                    print(f\"{'=' * 80}\")\n",
    "                    print(f\"   Total time: {int(total_time / 60)} min {total_time % 60} sec\")\n",
    "                    print(f\"   Gateway APIEvent  ({gateway_pipeline_id}) is ready!\")\n",
    "                    return True\n",
    "            else:\n",
    "                zero_count += 1   \n",
    "                if zero_count >= max_zero_count:\n",
    "                    break\n",
    "            # else:\n",
    "            #     print(f\"\\n❌ ERROR: Not all snapshots completed. Expected: {expected_table_count}, Actual: {snapshot_completed_total}\")\n",
    "            #     return False \n",
    "            \n",
    "            # Wait before next check\n",
    "            time.sleep(check_interval)\n",
    "            elapsed_time = int(time.time() - start_time)\n",
    "\n",
    "        print(f\"\\n❌ ERROR: Not all snapshots completed after checking for {max_wait_seconds}. Expected: {expected_table_count}, Actual: {snapshot_completed_total} ZeroCount={zero_count}\")\n",
    "        return False     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "739e2913-83ef-402d-9736-9eb66a92b277",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LakeflowGatewayAPI"
    }
   },
   "outputs": [],
   "source": [
    "class LakeflowGatewayAPI(LakeflowAPI):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def create_gateway_pipeline(self, p_pipeline_json, p_headers_dict=None):\n",
    "        print(\"\\nCreating gateway pipeline...\\n\")\n",
    "        v_template_pipeline_json =   {\n",
    "                                        \"pipeline_type\": \"INGESTION_GATEWAY\", \n",
    "                                        \"photon\": False,\n",
    "                                        \"serverless\": False,\n",
    "                                        \"continuous\": True,\n",
    "                                        \"name\": None,\n",
    "                                        \"catalog\": None,\n",
    "                                        \"target\": None,\n",
    "                                        \"clusters\": None,\n",
    "                                        # Gateway-specific configuration\n",
    "                                        \"gateway_definition\": {\n",
    "                                            \"connection_name\" : None,\n",
    "                                            \"gateway_storage_catalog\": None,\n",
    "                                            \"gateway_storage_schema\" : None\n",
    "                                        }\n",
    "                                    }\n",
    "        v_pipeline_json = v_template_pipeline_json | p_pipeline_json\n",
    "        self.validate_gateway_pipeline(v_template_pipeline_json, v_pipeline_json)\n",
    "        return self.create_pipeline(v_pipeline_json, p_headers_dict)\n",
    "    \n",
    "    def validate_gateway_pipeline(self, p_template_pipeline_json, p_pipeline_json):\n",
    "        \n",
    "        if not p_pipeline_json:\n",
    "            raise ValueError(\"Pipeline JSON is empty\")\n",
    "        if not isinstance(p_pipeline_json, dict):\n",
    "            raise ValueError(\"Pipeline JSON is not a dictionary\")\n",
    "        for key,value in p_template_pipeline_json.items():\n",
    "            if key not in p_pipeline_json or p_pipeline_json[key] is None:\n",
    "                raise ValueError(f\"Invalid key: {key} or Value not set\")\n",
    "            if key == \"gateway_definition\": \n",
    "                if not isinstance(value, dict):\n",
    "                    raise ValueError(\"Gateway definition must be a dictionary\")\n",
    "                for key2,value2 in value.items():\n",
    "                    if key2 not in p_pipeline_json[key] or p_pipeline_json[key][key2] is None:\n",
    "                        raise ValueError(f\"Invalid key: {key2} or Value not set\")\n",
    "        return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc8ede0-8b27-419e-8fcb-1d7139da72dc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LakeflowIngestionAPI"
    }
   },
   "outputs": [],
   "source": [
    "class LakeflowIngestionAPI(LakeflowAPI):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def create_ingestion_pipeline(self, p_pipeline_json, p_headers_dict=None):\n",
    "        v_template_pipeline_json =   {\n",
    "                                        \"pipeline_type\": \"MANAGED_INGESTION\",\n",
    "                                        \"name\": None,\n",
    "                                        \"photon\": True,\n",
    "                                        \"serverless\": True,\n",
    "                                        \"continuous\": False,\n",
    "                                        # Ingestion-specific configuration\n",
    "                                        \"ingestion_definition\": {\n",
    "                                            \"source_type\": \"SQLSERVER\",\n",
    "                                            \"ingestion_gateway_id\": None,\n",
    "                                            \"objects\": [\n",
    "                                                            {\n",
    "                                                                \"table\": {\n",
    "                                                                    \"source_catalog\": None,\n",
    "                                                                    \"source_schema\": None,\n",
    "                                                                    \"source_table\": None,\n",
    "                                                                    \"destination_catalog\": None,\n",
    "                                                                    \"destination_schema\": None,\n",
    "                                                                    \"destination_table\": None\n",
    "                                                                }\n",
    "                                                            }\n",
    "                                                        ]\n",
    "                                            }\n",
    "                                        }\n",
    "        v_pipeline_json = v_template_pipeline_json | p_pipeline_json\n",
    "        self.validate_ingestion_pipeline(v_template_pipeline_json, v_pipeline_json)\n",
    "        return self.create_pipeline(v_pipeline_json, p_headers_dict)\n",
    "    \n",
    "    def validate_ingestion_pipeline(self, p_template_pipeline_json, p_pipeline_json):\n",
    "        if not p_pipeline_json:\n",
    "            raise ValueError(\"Pipeline JSON is empty\")\n",
    "        if not isinstance(p_pipeline_json, dict):\n",
    "            raise ValueError(\"Pipeline JSON is not a dictionary\")\n",
    "        for key,value in p_template_pipeline_json.items():\n",
    "            if key not in p_pipeline_json or p_pipeline_json[key] is None:\n",
    "                raise ValueError(f\"Invalid key: {key} or Value not set\")\n",
    "            if key == \"ingestion_definition\": \n",
    "                if not isinstance(value, dict):\n",
    "                    raise ValueError(\"Ingestion definition must be a dictionary\")\n",
    "                for key2,value2 in value.items():\n",
    "                    if key2 not in p_pipeline_json[key] or p_pipeline_json[key][key2] is None:\n",
    "                        raise ValueError(f\"Invalid key: {key2} or Value not set\")\n",
    "                    if key2 == \"objects\": \n",
    "                        if not isinstance(value2, list):\n",
    "                            raise ValueError(\"Objects must be a list\")\n",
    "                        # for obj in value2:\n",
    "                        #     if not isinstance(obj, dict):\n",
    "                        #         raise ValueError(\"Object must be a dictionary\")\n",
    "                        #     for key3,value3 in obj.items():\n",
    "                        #         if key3 not in obj or obj[key3] is None:\n",
    "                        #             raise ValueError(f\"Invalid key: {key3} or Value not set\")\n",
    "                        #         if key3 == \"table\": \n",
    "                        #             if not isinstance(value3, dict):\n",
    "                        #                 raise ValueError(\"Table must be a dictionary\")\n",
    "                        #             for key4,value4 in value3.items():\n",
    "                        #                 if key4 not in obj[key3] or obj[key3][key4] is None:\n",
    "                        #                     raise ValueError(f\"Invalid key: {key4} or Value not set\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1010d3e6-33c0-4aa3-b784-b2e9205d3101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from datetime import datetime, timezone, timedelta\n",
    "# #Current time in UTC\n",
    "# now_utc = datetime.now(timezone.utc)\n",
    "# # substract 5 minutes\n",
    "# now_utc = now_utc - timedelta(minutes=5)\n",
    "# # Format with timezone offset and 'Z'\n",
    "# formatted = now_utc.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'  # ISO 8601 with milliseconds\n",
    "\n",
    "# print(formatted)\n",
    "\n",
    "# # If you want explicit offset instead of 'Z'\n",
    "# formatted_with_offset = now_utc.strftime('%Y-%m-%dT%H:%M:%S.%f%z')[:-3]\n",
    "# print(formatted_with_offset)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7475048966216303,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "LakeflowAPIHandler",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
