{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d148613f-c1a9-414c-9ce6-6fbd4048d5e6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Python Modules"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import traceback\n",
    "from pyspark.sql.functions import col, lower\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba039a16-4446-480d-986b-97814459fc8d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DeIdentification"
    }
   },
   "outputs": [],
   "source": [
    "class DeIdentification:\n",
    "    def __init__(self, p_source_catalog, p_source_schema, p_source_table_list, p_target_catalog, p_target_schema, p_mode='overwrite', p_create_if_not_exists=True):\n",
    "        self.params_dict = {\n",
    "            'source_catalog': p_source_catalog,\n",
    "            'source_schema': p_source_schema,\n",
    "            'source_table_list': p_source_table_list,\n",
    "            'target_catalog': p_target_catalog,\n",
    "            'target_schema': p_target_schema,\n",
    "            'mode': p_mode,\n",
    "            'create_if_not_exists': p_create_if_not_exists\n",
    "        }\n",
    "\n",
    "        self.process_max_workers = 5\n",
    "\n",
    "    def set_process_max_workers(self, process_max_workers):\n",
    "        self.process_max_workers = process_max_workers\n",
    "\n",
    "    def get_process_max_workers(self):\n",
    "        return self.process_max_workers\n",
    "    \n",
    "    def process(self):\n",
    "\n",
    "        v_thread_errors = []       # To collect all exceptions\n",
    "        v_thread_results = []      # To collect successful result\n",
    "        v_return_dict = {'Total': 0, 'ExecutionFailed' : 0, 'Success': 0}\n",
    "        print(\"Processing...\")\n",
    "        table_array = []\n",
    "        for table in self.params_dict['source_table_list']:\n",
    "            #print(f\"Processing table {table}...\")\n",
    "\n",
    "            table_dict = {}\n",
    "            table_dict['source_table'] = self.params_dict['source_catalog'] + '.' + self.params_dict['source_schema'] + '.' + table['source_table']\n",
    "            table_dict['target_table'] =  self.params_dict['target_catalog'] + '.' +  self.params_dict['target_schema'] + '.' + table['source_table']\n",
    "            table_dict['target_mode'] =  table['mode'] if 'mode' in table else self.params_dict['mode']\n",
    "            table_dict['target_create_if_not_exists'] = table['create_if_not_exists'] if 'create_if_not_exists' in table else self.params_dict['create_if_not_exists']\n",
    "\n",
    "            table_dict['keys'] = table['keys'] if 'keys' in table else None\n",
    "\n",
    "            table_dict['ProcessIdentifier'] = table_dict['source_table'] + '|' + table_dict['target_table']\n",
    "\n",
    "            table_array.append(table_dict)\n",
    "\n",
    "        v_return_dict['Total'] = len(table_array)\n",
    "\n",
    "        if len(table_array) > 0:\n",
    "            with ThreadPoolExecutor(max_workers=self.get_process_max_workers()) as executor:\n",
    "                futures = {executor.submit(self.process_table, obj): obj for obj in table_array}\n",
    "                for future in as_completed(futures):\n",
    "                    obj = futures[future]\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        v_thread_results.append(str(obj['ProcessIdentifier']) + ' \\nMessage=Data de-identified and copied.')\n",
    "                        print(f\"Thread Successes: \", v_thread_results)\n",
    "                    except Exception as e:\n",
    "                        v_thread_errors.append(str(obj['ProcessIdentifier']) + ' \\nMessage=' + str(e) + ' \\nTraceback=' + traceback.format_exc().replace('\\n',''))\n",
    "                        print(f\"Thread Errors: \", v_thread_errors)\n",
    "\n",
    "        v_return_dict['Success'] = len(v_thread_results)\n",
    "        v_return_dict['ExecutionFailed'] = len(v_thread_errors)\n",
    "        v_return_dict['ExecutionFailures'] = v_thread_errors\n",
    "        v_return_dict['SuccessResults'] = v_thread_results\n",
    "\n",
    "        print(\"\\n\\nSummary:\\n\")\n",
    "        pprint.pprint(v_return_dict, indent=4)\n",
    "\n",
    "        if len(v_thread_errors) > 0:\n",
    "            print(f\"Thread Errors: \", v_thread_errors)\n",
    "            raise Exception(v_return_dict)\n",
    "        \n",
    "        return v_return_dict    \n",
    "\n",
    "    def process_table(self, table_dict):\n",
    "        return self.DataMaskingTable(table_dict['source_table'], table_dict['target_table'], table_dict['target_mode'], table_dict['keys'], None, table_dict['target_create_if_not_exists'])\n",
    "\n",
    "    def DataMaskingTableValidation(self, p_source_table, p_target_table, p_target_mode, p_keys, p_rule_table, p_create_target_if_not_exists):\n",
    "    \n",
    "        return_dict = {'validation_message': None, 'source_table_catalog' : None, 'source_table_schema': None, 'source_table': None, 'target_table_exists': False, 'p_rule_table' : None}\n",
    "        validation_message = ''\n",
    "        \n",
    "\n",
    "        if p_rule_table is None:\n",
    "            p_rule_table = return_dict['p_rule_table'] = 'db_config.cfgdatamasking2'\n",
    "\n",
    "        if not spark.catalog.tableExists(p_rule_table):\n",
    "            validation_message += f\"Rule table {p_rule_table} does not exist.\\n\"\n",
    "\n",
    "        if not p_target_table or (not p_create_target_if_not_exists and not spark.catalog.tableExists(p_target_table)):\n",
    "            validation_message += f\"Target table {p_target_table} does not exist.\\n\"\n",
    "        else:        \n",
    "            return_dict['target_table_exists'] = spark.catalog.tableExists(p_target_table)\n",
    "\n",
    "        if p_target_mode.lower() not in ['merge', 'overwrite', 'append']:\n",
    "            validation_message += f\"Target mode {p_target_mode} is not supported. Please use 'merge', 'overwrite', or 'append'.\"\n",
    "\n",
    "        if p_target_mode.lower() == 'merge' and not p_keys:\n",
    "            validation_message += f\"Target mode {p_target_mode} requires keys to be specified as a list or a string (comma separate).\\n\"    \n",
    "\n",
    "        if spark.catalog.tableExists(p_source_table):\n",
    "            source_table_split_array = p_source_table.split('.')\n",
    "            if len(source_table_split_array) == 3:\n",
    "                return_dict['source_table_catalog'] = source_table_split_array[0] \n",
    "                return_dict['source_table_schema'] = source_table_split_array[1]\n",
    "                return_dict['source_table'] = source_table_split_array[2]   \n",
    "            elif len(source_table_split_array) == 2:\n",
    "                return_dict['source_table_schema'] = source_table_split_array[0]\n",
    "                return_dict['source_table'] = source_table_split_array[1]  \n",
    "            else:\n",
    "                pass       \n",
    "            #print(f\"source_table_catalog: {source_table_catalog}, source_table_schema: {source_table_schema}, source_table: {source_table}, rule_table: {p_rule_table}\")\n",
    "        else:\n",
    "            validation_message += f\"Source table {p_source_table} does not exist.\\n\"\n",
    "\n",
    "        # Source and Target Table should be different\n",
    "        if p_source_table == p_target_table:\n",
    "            validation_message += f\"Source table {p_source_table} and Target table {p_target_table} are the same.\\n\"  \n",
    "\n",
    "        return_dict['validation_message'] = validation_message\n",
    "        \n",
    "        return return_dict    \n",
    "\n",
    "    def DataMaskingTable(self, p_source_table, p_target_table, p_target_mode, p_keys=None, p_rule_table=None, p_create_target_if_not_exists=False):\n",
    "        \"\"\"\n",
    "        This function is used to mask the data in the target table based on the rules in the rule table.\n",
    "        The rule table is a dataframe with the following columns:\n",
    "        - column_name: the name of the column to be masked\n",
    "        - rule: the rule to be applied to the column\n",
    "        - value: the value to be used in the rule\n",
    "        \"\"\"\n",
    "        return_dict = {'status': 'failed', 'message': None, 'source_table': p_source_table, 'target_table': p_target_table,\n",
    "                    'num_affected_rows': None, 'num_updated_rows': None, 'num_inserted_rows': None, 'num_deleted_rows': None}\n",
    "\n",
    "        validation_return_dict = self.DataMaskingTableValidation(p_source_table, p_target_table, p_target_mode, p_keys, p_rule_table, p_create_target_if_not_exists)\n",
    "\n",
    "        print(validation_return_dict)\n",
    "\n",
    "        if validation_return_dict['validation_message']:\n",
    "            return_dict['message'] = validation_return_dict['validation_message']\n",
    "            raise Exception(validation_return_dict['validation_message'])\n",
    "        else:\n",
    "\n",
    "            p_rule_table = validation_return_dict['p_rule_table']\n",
    "            source_table_schema = validation_return_dict['source_table_schema']\n",
    "            source_table = validation_return_dict['source_table']\n",
    "            source_table_catalog = validation_return_dict['source_table_catalog']\n",
    "            target_table_exists = validation_return_dict['target_table_exists']\n",
    "            \n",
    "            \n",
    "            #Get List of Columns from Source Table\n",
    "            source_table_columns_list = spark.sql(f\"select * from {p_source_table} limit 0\").columns\n",
    "            source_table_columns_dict = {column.lower(): column for column in source_table_columns_list}\n",
    "            source_table_columns_dict_keys = list(source_table_columns_dict.keys())\n",
    "            print(\"\\nDict Keys\", source_table_columns_dict_keys)\n",
    "            print(\"\\nInitial Column Dict:\", source_table_columns_dict)\n",
    "            sql_text = f\"\"\"  select *, row_number() over (partition by DataMaskingColumn order by priority asc) as row_priority\n",
    "                                            from\n",
    "                                            (\n",
    "                                                select DataMaskingColumn, DataMaskingString, DataMaskingSchema, DataMaskingTable, \n",
    "                                                CASE \n",
    "                                                    WHEN POSITION('%' IN DataMaskingSchema) == 0 and POSITION('%' IN DataMaskingTable) == 0 THEN 1\n",
    "                                                    WHEN POSITION('%' IN DataMaskingSchema) > 0 and POSITION('%' IN DataMaskingTable) == 0 THEN 2\n",
    "                                                    WHEN POSITION('%' IN DataMaskingSchema) == 0 and POSITION('%' IN DataMaskingTable) > 0 THEN 3\n",
    "                                                    WHEN POSITION('%' IN DataMaskingSchema) > 0 and POSITION('%' IN DataMaskingTable) > 0 THEN 4\n",
    "                                                    ELSE 5\n",
    "                                                END as priority\n",
    "                                                from {p_rule_table} \n",
    "                                                where '{source_table_schema}' ilike DataMaskingSchema and '{source_table}' ilike DataMaskingTable\n",
    "                                            )\n",
    "                                    \"\"\"\n",
    "        \n",
    "            df_rule_table = spark.sql(sql_text).filter((lower(col('DataMaskingColumn')).isin(source_table_columns_dict_keys)) & (col('row_priority') == 1))\n",
    "            print(sql_text)\n",
    "            df_rule_table.show(truncate=False) \n",
    "            rule_dict = {row['DataMaskingColumn'].lower(): row['DataMaskingString'] for row in df_rule_table.collect()}\n",
    "            print(\"\\nRule Column Dict:\", rule_dict)\n",
    "            source_table_columns_dict.update(rule_dict)\n",
    "            print(\"\\nFinal Column Dict:\",source_table_columns_dict)\n",
    "            source_table_select_str = 'select ' + ', '.join(source_table_columns_dict.values()) + ' from {0}'.format(p_source_table)\n",
    "            print(\"\\nSourece Table Select String\", source_table_select_str)\n",
    "\n",
    "            if validation_return_dict['target_table_exists'] == False and p_create_target_if_not_exists == True:\n",
    "                sql_text = f\"\"\"CREATE TABLE IF NOT EXISTS {p_target_table} AS SELECT * FROM {p_source_table} where 1=0;\"\"\"\n",
    "                print(sql_text)\n",
    "                spark.sql(sql_text)\n",
    "\n",
    "            if p_target_mode.lower() == 'merge':\n",
    "                source_table_keys_dict = {column: column for column in p_keys} if isinstance(p_keys, list) else {column: column for column in p_keys.split(',')}\n",
    "                merge_sql_str = \"Merge into {0} as target using ({1}) as source on {2} when matched then update set {3} when not matched then insert ({4}) values ({5})\"\n",
    "                merge_sql_str = merge_sql_str.format(p_target_table, \n",
    "                                                    source_table_select_str, \n",
    "                                                    ' and '.join([f\"target.{column} = source.{column}\" for column in source_table_keys_dict]), \n",
    "                                                    ', '.join([f\"target.{column} = source.{column}\" for column in source_table_columns_dict]), \n",
    "                                                    ', '.join(source_table_columns_dict), \n",
    "                                                    ', '.join(source_table_columns_dict.keys())\n",
    "                                                    )\n",
    "                print(\"\\nMerge String\", merge_sql_str)\n",
    "                df_merge = spark.sql(merge_sql_str)\n",
    "                df_merge.show()\n",
    "                df_merge_result = df_merge.collect()\n",
    "                \n",
    "                return_dict['num_affected_rows'] = df_merge_result[0]['num_affected_rows']\n",
    "                return_dict['num_updated_rows'] = df_merge_result[0]['num_updated_rows']\n",
    "                return_dict['num_inserted_rows'] = df_merge_result[0]['num_inserted_rows']\n",
    "                return_dict['num_deleted_rows'] = df_merge_result[0]['num_deleted_rows']\n",
    "                return_dict['status'] = 'success'\n",
    "            elif p_target_mode.lower() in ['append', 'overwrite']:\n",
    "                df_target = spark.sql(source_table_select_str)\n",
    "                df_target.write.format(\"delta\").mode(p_target_mode).saveAsTable(p_target_table)\n",
    "                return_dict['status'] = 'success'\n",
    "            \n",
    "                \n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a9ca7c-315c-4ca9-8069-318ca4ec8493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# select \n",
    "# --SourceServerName2 as source_catalog, \n",
    "# distinct\n",
    "# lower(StepName) as source_table, \n",
    "# CDCKeyColumns as keys,\n",
    "# 'merge' as mode,\n",
    "#  1 as create_if_not_exists\n",
    "# from [CFG].[vw_TableProcessList] where SourceServerName1 = 'LEWVPXAPDBRP15'\n",
    "# and SourceServerName2 = 'clm_ods_lewvpxapdbrp15_cdc_04'\n",
    "# FOR JSON AUTO"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8956673534509498,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DeIdentificationHandler",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
