{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b8fe8d1-c370-4f5d-b87a-94261cf6ace1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Process Config Classes"
    }
   },
   "outputs": [],
   "source": [
    "%run ../classhandlers/ProcessConfigHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1181789f-4ff2-4801-b46f-58a732479cbb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Unity Catalog Classes"
    }
   },
   "outputs": [],
   "source": [
    "%run ../classhandlers/UnityCatalogHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d81afec0-9f2b-4b36-b525-7277079b10b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Python Modules"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import traceback\n",
    "import uuid\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql.functions import col, lower, lit, current_timestamp, current_user, current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78969aab-9a81-4cf1-a0e4-db5c781837b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ClientDataSplit"
    }
   },
   "outputs": [],
   "source": [
    "class ClientDataSplit:\n",
    "    def __init__(self, p_environment,  p_internal_product_id, p_source_server_name, p_source_database_name=None, p_internal_client_id=None, p_internal_facility_id=None,p_ingestion_pipeline_name=None):\n",
    "        self.params_dict = {\n",
    "            'p_environment': p_environment,\n",
    "            'p_internal_product_id': p_internal_product_id,\n",
    "            'p_source_server_name': p_source_server_name,\n",
    "            'p_source_database_name': p_source_database_name,\n",
    "            'p_internal_client_id': p_internal_client_id,\n",
    "            'p_internal_facility_id': p_internal_facility_id,\n",
    "            'p_ingestion_pipeline_name': p_ingestion_pipeline_name\n",
    "        }\n",
    "\n",
    "        self.process_max_workers = 50\n",
    "        self.trackeback_length = 1000\n",
    "        self.max_retries = 3\n",
    "        self.retry_delay = 60\n",
    "        self.retry_delay_list = self.set_retry_delay_list()\n",
    "\n",
    "    def set_process_max_workers(self, process_max_workers):\n",
    "        self.process_max_workers = process_max_workers\n",
    "\n",
    "    def get_process_max_workers(self):\n",
    "        return self.process_max_workers\n",
    "    \n",
    "    def set_traceback_length(self, traceback_length):\n",
    "        self.trackeback_length = traceback_length\n",
    "\n",
    "    def get_traceback_length(self):\n",
    "        return self.trackeback_length\n",
    "    \n",
    "    def set_max_retries(self, max_retries):\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "    def get_max_retries(self):\n",
    "        return self.max_retries\n",
    "    \n",
    "    def set_retry_delay(self, retry_delay):\n",
    "        self.retry_delay = retry_delay\n",
    "        self.retry_delay_list = self.set_retry_delay_list()\n",
    "\n",
    "    def get_retry_delay(self):\n",
    "        return self.retry_delay\n",
    "    \n",
    "    def get_random_retry_delay(self):\n",
    "        return random.choice(self.retry_delay_list)\n",
    "    \n",
    "    def set_retry_delay_list(self):\n",
    "        return [int(self.retry_delay*i) for i in [.25,.50,1,1.25,1.50]]\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        v_thread_errors = []       # To collect all exceptions\n",
    "        v_thread_results = []      # To collect successful results\n",
    "        v_validation_errors = []\n",
    "        v_skipped_results = []\n",
    "        v_config_row_array = []\n",
    "        v_config_unique_pipelines_dict = {}\n",
    "        v_pipelines_failed_list = []\n",
    "        v_return_dict = {'Total': 0, 'Skipped': 0, 'ValidationFailed' : 0, 'ExecutionFailed' : 0, 'Success': 0}\n",
    "        # Get Unity Catalog and Managed Location Root Path\n",
    "        process_config = ProcessConfigData(self.params_dict['p_environment'])\n",
    "        v_unity_catalog = process_config.get_config_attribute_value('AnalyticsUnityCatalog')\n",
    "        v_managed_location_root_path = process_config.get_config_attribute_value('AdlsAnalyticsFullpathUri')\n",
    "        # Load Config Data for Client Split\n",
    "        df_config_data_rows = process_config.get_table_split_list(\n",
    "                                                                    self.params_dict['p_internal_product_id'],\n",
    "                                                                    self.params_dict['p_source_server_name'], \n",
    "                                                                    self.params_dict['p_source_database_name'],\n",
    "                                                                    self.params_dict['p_internal_client_id'],\n",
    "                                                                    self.params_dict['p_internal_facility_id'],\n",
    "                                                                    self.params_dict['p_ingestion_pipeline_name']\n",
    "                                                                )\n",
    "        if df_config_data_rows is None or df_config_data_rows.count() == 0:\n",
    "            raise Exception(f\"Unable to find Configuration rows for {self.params_dict}\")\n",
    "        else:\n",
    "            v_return_dict['Total'] = df_config_data_rows.count()\n",
    "            #print(df_config_data_rows, \"---\\n---\", df_config_data_rows.count())\n",
    "            #df_config_data_rows.display()\n",
    "\n",
    "        print(f\"Start - Validation of {v_return_dict['Total']} config Rows....\")\n",
    "        for row in df_config_data_rows.collect():\n",
    "            if row['Status'] == 'S':\n",
    "                v_return_dict['Skipped'] += 1\n",
    "                v_skipped_message = f\"Warning: SKipping - Configuration row ({row['ProcessIdentifier']}) is marked as Success.\"\n",
    "                v_skipped_results.append(v_skipped_message)\n",
    "                #print(v_skipped_message)\n",
    "            else:\n",
    "                \n",
    "                v_config_row_dict = row.asDict()\n",
    "                v_config_row_dict['DestinationCatalog'] = v_unity_catalog\n",
    "                v_validation_message = self.validate_config_row(v_config_row_dict)\n",
    "\n",
    "                if v_validation_message is None:\n",
    "                    v_config_row_dict['Status'] = 'Pending'\n",
    "                    v_config_row_dict['DestinationCatalog'] = v_unity_catalog\n",
    "                    v_config_row_dict['ManagedLocationRootPath'] = v_managed_location_root_path\n",
    "                    v_config_row_dict['process_config'] = process_config\n",
    "                    v_config_row_array.append(v_config_row_dict)\n",
    "\n",
    "                    # Record Unique Pipeline Names for Status Facility Update\n",
    "                    v_config_unique_pipelines_dict[v_config_row_dict['IngestionPipelineName']] = \\\n",
    "                                                    {   'PipelineName': v_config_row_dict['IngestionPipelineName'], \\\n",
    "                                                        'InternalProductId' : v_config_row_dict['InternalProductId'], \\\n",
    "                                                        'DataSourceId' : v_config_row_dict['DataSourceId'], \\\n",
    "                                                        'StepType' : 'Ods', \\\n",
    "                                                        'Status': 'P'\n",
    "                                                    }\n",
    "\n",
    "                    #print(f\"Client Data Split will proceed for {row['ProcessIdentifier']}\")\n",
    "                else:\n",
    "                    v_return_dict['ValidationFailed'] += 1\n",
    "                    v_validation_message = f\"Client Data Split will NOT proceed for {row['ProcessIdentifier']}: {v_validation_message}\"\n",
    "                    v_validation_errors.append(v_validation_message)\n",
    "                    #print(v_validation_message)\n",
    "        print(\"End - Validation of config Rows....\")\n",
    "\n",
    "        if len(v_config_row_array) == 0:\n",
    "            v_return_dict['ValidationFailures'] = v_validation_errors\n",
    "            v_return_dict['SkippedResults'] = v_skipped_results\n",
    "        else:\n",
    "            # Set Pipeline Status as Pending\n",
    "            process_config.set_pipeline_process_status_list(v_config_unique_pipelines_dict.values())\n",
    "\n",
    "            # Launch Threads Processing\n",
    "            with ThreadPoolExecutor(max_workers=self.get_process_max_workers()) as executor:\n",
    "                futures = {executor.submit(self.process_config_row, obj): obj for obj in v_config_row_array}\n",
    "                for future in as_completed(futures):\n",
    "                    obj = futures[future]\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        v_thread_results.append(str(obj['ProcessIdentifier']) + ' \\nMessage=Client Data Split Successfully Processed. ' + str(result))\n",
    "                        #print(f\"Thread Successes: \", v_thread_results)\n",
    "                    except Exception as e:\n",
    "                        v_pipelines_failed_list.append(obj['IngestionPipelineName'])\n",
    "                        v_thread_errors.append(str(obj['ProcessIdentifier']) + ' \\nMessage=' + str(e).replace('\\n','')[0:self.get_traceback_length()] + ' \\nTraceback=' + traceback.format_exc().replace('\\n','')[0:self.get_traceback_length()])\n",
    "                        #print(f\"Thread Errors: \", v_thread_errors)\n",
    "\n",
    "            # Get Unique List of Failed Pipelines And Update Dictionary\n",
    "            v_pipelines_failed_list = list(set(v_pipelines_failed_list))\n",
    "            for k in v_config_unique_pipelines_dict.keys():\n",
    "                v_config_unique_pipelines_dict[k]['Status'] = 'C' if k not in v_pipelines_failed_list else 'F'\n",
    "            \n",
    "            # Set Final Pipeline Status\n",
    "            process_config.set_pipeline_process_status_list(v_config_unique_pipelines_dict.values())\n",
    "\n",
    "        v_return_dict['Success'] = len(v_thread_results)\n",
    "        v_return_dict['SuccessResults'] = [] #v_thread_results\n",
    "        v_return_dict['SkippedResults'] = v_skipped_results\n",
    "        v_return_dict['ValidationFailures'] = v_validation_errors\n",
    "        v_return_dict['ExecutionFailed'] = len(v_thread_errors)\n",
    "        v_return_dict['ExecutionFailures'] = v_thread_errors\n",
    "\n",
    "        print(\"\\n\\nSummary:\\n\")\n",
    "        pprint.pprint(v_return_dict, indent=4, compact=True, sort_dicts=False, width=10000)\n",
    "\n",
    "        if len(v_thread_errors) + len(v_validation_errors) > 0:\n",
    "            raise Exception(f\"Execution errors: {len(v_thread_errors)} , Validation Errors: {len(v_validation_errors)}  Skipped: {len(v_skipped_results)}, Success: {len(v_thread_results)} were encountered during processing. See exception for details.\")\n",
    "        elif len(v_config_row_array) == 0:\n",
    "            raise Exception(f\"Unable to find valid runnable configuration(s) for {self.params_dict} - Skipped: {len(v_skipped_results)}\")\n",
    "        \n",
    "        \n",
    "        return v_return_dict\n",
    "\n",
    "    def process_config_row(self, p_config_row_dict):\n",
    "\n",
    "        print(\"Start - Processing: \", p_config_row_dict['ProcessIdentifier'])\n",
    "        v_attempt_message = ''\n",
    "        v_return_dict = {}\n",
    "        v_retry_count = 0\n",
    "        v_start = datetime.now()\n",
    "        v_row_status_dict = {'InternalProductId': p_config_row_dict['InternalProductId'],\n",
    "                             'InternalClientId': p_config_row_dict['InternalClientId'],\n",
    "                             'InternalFacilityId': p_config_row_dict['InternalFacilityId'],\n",
    "                             'DataSourceId': p_config_row_dict['DataSourceId'],\n",
    "                             'Status': 'F',\n",
    "                             'StepType': 'Ods',\n",
    "                             'StepName': p_config_row_dict['DestinationTable'],\n",
    "                             'PipelineName': p_config_row_dict['IngestionPipelineName']}\n",
    "\n",
    "        # Define Audit Columns Dictionaries\n",
    "        v_create_audit_dict = {'DateTimeCreated': 'current_timestamp', 'CreatedByUser': 'current_user'}\n",
    "        v_update_audit_dict = {'DateTimeLastModified': 'current_timestamp', 'ModifiedByUser': 'current_user'}\n",
    "        \n",
    "        # Default Watermark Value if not passed....\n",
    "        v_default_watermark_value = '2000-01-01'\n",
    "        v_watermark_value = p_config_row_dict['WatermarkValue'] if p_config_row_dict['WatermarkValue'] else v_default_watermark_value\n",
    "\n",
    "        # Compute Source Table and Query - Load into Dataframe base on Query Template\n",
    "        v_full_source_schema = p_config_row_dict['DestinationCatalog']+'.'+p_config_row_dict['DestinationSchema']\n",
    "        v_full_source_table =  v_full_source_schema+'.'+p_config_row_dict['DestinationTable']\n",
    "        sql_source_query = p_config_row_dict['IncrementalExtractQuery'] if p_config_row_dict['IsHistorical'] == 0 else p_config_row_dict['HistoricalExtractQuery']\n",
    "        sql_source_query = sql_source_query.format(  UC_SchemaName=v_full_source_schema, \n",
    "                                                    TableName=p_config_row_dict['DestinationTable'],\n",
    "                                                    SiteId=p_config_row_dict['SourceFacilityId'],\n",
    "                                                    SourceFacilityId=p_config_row_dict['SourceFacilityId'],\n",
    "                                                    WatermarkValue=f\"'{v_watermark_value}'\",\n",
    "                                                    SourceFacilityCode=f\"'{p_config_row_dict['SourceFacilityCode']}'\",\n",
    "                                                    SourceClientId=p_config_row_dict['SourceClientId'],\n",
    "                                                    SourceClientCode=f\"'{p_config_row_dict['SourceClientCode']}'\",\n",
    "                                                    InternalClientId=p_config_row_dict['InternalClientId']\n",
    "                                                )\n",
    "        # Load Source Table based on Query\n",
    "        #print(f\"Executing Query Data Split: {sql_source_query}\")\n",
    "        df_source = spark.sql(sql_source_query) \n",
    "\n",
    "        # Build Filter Dictionary\n",
    "        v_filter_dict = {}\n",
    "        if p_config_row_dict['InternalClientId'] > 0:\n",
    "            v_filter_dict['InternalClientId'] = p_config_row_dict['InternalClientId']\n",
    "        if p_config_row_dict['InternalFacilityId'] > 0:\n",
    "            v_filter_dict['InternalFacilityId'] = p_config_row_dict['InternalFacilityId']\n",
    "        v_ids_filter = ' and '.join([ k+'='+str(v) for k,v in v_filter_dict.items()])\n",
    "        \n",
    "        # Add Internal Client/Facility\n",
    "        if 'InternalClientId' in v_filter_dict: \n",
    "                df_source = df_source.withColumn('InternalClientId', lit(v_filter_dict['InternalClientId']))\n",
    "        if 'InternalFacilityId' in v_filter_dict:\n",
    "            df_source = df_source.withColumn('InternalFacilityId', lit(v_filter_dict['InternalFacilityId']))\n",
    "\n",
    "        # Add Audit Columns\n",
    "        df_source = df_source.withColumn('DateTimeCreated', current_timestamp()) \\\n",
    "                            .withColumn('CreatedByUser', current_user()) \\\n",
    "                            .withColumn('DateTimeLastModified', current_timestamp()) \\\n",
    "                            .withColumn('ModifiedByUser', current_user()) \\\n",
    "                            .withColumn('DataSourceSchemaName', lit(p_config_row_dict['DestinationSchema']))\n",
    "        \n",
    "        # Build Client Table Full Name\n",
    "        v_full_client_table = p_config_row_dict['DestinationCatalog']+'.'+p_config_row_dict['ClientODSSchema']+'.'+p_config_row_dict['ClientODSTable']\n",
    "\n",
    "        #TESTING FAILURES\n",
    "        # if p_config_row_dict['DestinationTable'] == 'ClaimDataUpdated':\n",
    "        #     raise Exception(\"Test Exception\")\n",
    "        \n",
    "        v_retry_status = \"Failed\"\n",
    "        while v_retry_count < self.get_max_retries():\n",
    "            try:\n",
    "                # Create Client Schema if not exists\n",
    "                v_location_path = p_config_row_dict['ManagedLocationRootPath']+'/'+p_config_row_dict['ClientODSSchema']\n",
    "                uc = UnityCatalogTableOperations()\n",
    "                uc.create_schema(p_config_row_dict['DestinationCatalog'], p_config_row_dict['ClientODSSchema'],v_location_path)\n",
    "\n",
    "                # Create Table if does not exists ......\n",
    "                if not spark.catalog.tableExists(v_full_client_table):\n",
    "                    # # Partition By InternalFacilityId if exists\n",
    "                    # if 'InternalFacilityId' in v_filter_dict:\n",
    "                    #     df_source.write.partitionBy('InternalFacilityId').saveAsTable(v_full_client_table)\n",
    "                    # else:                \n",
    "                    #     df_source.write.saveAsTable(v_full_client_table)\n",
    "\n",
    "                    # Create Table with deletetion vectors enabled.\n",
    "                    df_source.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .option(\"delta.enableDeletionVectors\", \"true\") \\\n",
    "                        .saveAsTable(v_full_client_table)\n",
    "\n",
    "                    # Add Liquid Clustering By AUTO\n",
    "                    uc.table_add_cluster_key(v_full_client_table)\n",
    "\n",
    "                    # Add Primary Keys if exists with RELY Option\n",
    "                    if p_config_row_dict['KeyColumns']:\n",
    "                        v_keys_list = p_config_row_dict['KeyColumns'].split(',')\n",
    "                        uc.table_add_primary_key(v_full_client_table, v_keys_list, True)\n",
    "\n",
    "                    v_return_dict['num_created_rows'] = df_source.count()\n",
    "\n",
    "                else:\n",
    "                    # Delete Rows on Client Table if is Historical\n",
    "                    if p_config_row_dict['IsHistorical'] == 1:\n",
    "                        v_delete_filter = f\" WHERE {v_ids_filter}\" if len(v_ids_filter) > 0 else ''\n",
    "                        spark.sql(f\"DELETE FROM {v_full_client_table} {v_delete_filter};\")\n",
    "\n",
    "                    # Merge if Keys Exists\n",
    "                    if p_config_row_dict['KeyColumns']:\n",
    "                        # Create Temp View from Dataframe\n",
    "                        unique_id = uuid.uuid4().hex\n",
    "                        v_vw_source_data = f\"tmp_{p_config_row_dict['ClientODSSchema']}_{p_config_row_dict['ClientODSTable']}_{unique_id}\"\n",
    "                        df_source.createOrReplaceTempView(v_vw_source_data)\n",
    "                        # Construct Key List\n",
    "                        v_key_list = p_config_row_dict['KeyColumns'].split(',')\n",
    "                        if 'InternalFacilityId' in v_filter_dict:\n",
    "                            v_key_list.append('InternalFacilityId')\n",
    "                            \n",
    "                        # Call Merge Function\n",
    "                        v_retun_merge_dict = uc.merge_table(v_vw_source_data, v_full_client_table, v_ids_filter, v_key_list, v_create_audit_dict, v_update_audit_dict)\n",
    "                        \n",
    "                        # Update Return dictionary\n",
    "                        v_return_dict.update(v_retun_merge_dict)\n",
    "\n",
    "                    # If No Keys, Append data if 'Watermark' specified otherwise overwrite\n",
    "                    else:\n",
    "                        mode = 'append' if 'WaterMark' in p_config_row_dict['IncrementalExtractQuery'] else 'overwrite' \n",
    "                        df_source.write.mode(mode).saveAsTable(v_full_client_table)\n",
    "\n",
    "                    v_retry_status = \"Success\"\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                v_retry_count += 1\n",
    "                v_delay = self.get_random_retry_delay()\n",
    "                v_attempt_message = f\"{e}\"\n",
    "                print(f\"Attempt {v_retry_count} failed (Sleep {v_delay} seconds): {p_config_row_dict['ProcessIdentifier']} {e}\")\n",
    "                time.sleep(v_delay)\n",
    "                \n",
    "        v_return_dict['duration'] = round((datetime.now() - v_start).total_seconds())\n",
    "        v_return_dict['status'] = v_retry_status\n",
    "        v_return_dict['retry_count'] = v_retry_count \n",
    "\n",
    "        # On Failure - Update Status to 'F' and raise Error\n",
    "        if v_retry_status != 'Success':\n",
    "            p_config_row_dict['process_config'].set_data_load_process_status_detail(v_row_status_dict, ) \n",
    "            raise Exception(f\"End - All {v_retry_count} Attempts Failed - Processing ({v_return_dict['duration']}s): {p_config_row_dict['ProcessIdentifier']} - {v_attempt_message}\")\n",
    "                      \n",
    "        # On Success - Update Status to 'S'\n",
    "        v_row_status_dict['Status'] = 'S'\n",
    "        p_config_row_dict['process_config'].set_data_load_process_status_detail(v_row_status_dict) \n",
    "\n",
    "        print(f\"End - Processing ({v_return_dict['duration']}s): \", p_config_row_dict['ProcessIdentifier'])\n",
    "        return v_return_dict\n",
    "\n",
    "    def validate_config_row(self, p_config_row_dict):\n",
    "        validation_message = ''\n",
    "        #print(\"Validation\\n\", p_config_row_dict)\n",
    "        if p_config_row_dict['IncrementalExtractQuery'] is None:\n",
    "            validation_message += 'IncrementalExtractQuery is required\\n'\n",
    "        if p_config_row_dict['HistoricalExtractQuery'] is None:\n",
    "            validation_message += 'HistoricalExtractQuery is required\\n'\n",
    "        if p_config_row_dict['ClientODSSchema'] is None:\n",
    "            validation_message += 'ClientODSSchema is required\\n'\n",
    "        if p_config_row_dict['ClientODSTable'] is None:\n",
    "            validation_message += 'ClientODSTable is required\\n'\n",
    "        if p_config_row_dict['DestinationSchema'] is None:\n",
    "            validation_message += 'DestinationSchema is required\\n'\n",
    "        if p_config_row_dict['DestinationTable'] is None:\n",
    "            validation_message += 'DestinationTable is required\\n'\n",
    "        if 'DestinationCatalog' not in p_config_row_dict or \\\n",
    "            p_config_row_dict['DestinationCatalog'] is None:\n",
    "            validation_message += 'DestinationCatalog is required\\n'\n",
    "\n",
    "        if validation_message == \"\":\n",
    "            v_str = p_config_row_dict['DestinationCatalog']+'.'+p_config_row_dict['DestinationSchema']+'.'+p_config_row_dict['DestinationTable']\n",
    "            if not spark.catalog.tableExists(v_str):\n",
    "                validation_message += f\"Source Streaming Table {v_str} is missing\\n\"\n",
    "        \n",
    "        return validation_message if validation_message != '' else None\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ff953e-ca78-48ba-aa90-d9909d251303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# obj = ClientDataSplit('dev', 27, 'lewvpalyedb04.nthext.com', None, None, None, 'ingst_sqlcdc_global_lewvpalyedb04_x3domain1')\n",
    "# obj.process()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8155127605544155,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ClientDataHandler",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
