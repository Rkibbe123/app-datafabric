{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f5670c-2b14-42d3-8025-767aac62412b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use once DataFabricCommonFunctions in Simoun's Workspace is promoted to this folder\n",
    "# %run \"./DataFabricCommonFunctions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22c2588-840f-4c6c-a1d2-4510149c66d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./DataFabricCommonFunctions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98da506c-17e7-4569-bb7d-87f96a7ba45d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from pyspark.sql.utils import AnalysisException\n",
    "# from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "# def append_or_create_table(df, target_table):\n",
    "#     \"\"\"Append to an existing table or create a new one.\"\"\"\n",
    "#     try:\n",
    "#         spark.read.table(target_table)\n",
    "#         df.write.format(\"delta\").mode(\"append\").option(\n",
    "#             \"mergeSchema\", \"true\"\n",
    "#         ).saveAsTable(target_table)\n",
    "#         return True\n",
    "#     except AnalysisException:\n",
    "#         df.write.format(\"delta\").mode(\"overwrite\").option(\n",
    "#             \"overwriteSchema\", \"true\"\n",
    "#         ).saveAsTable(target_table)\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {target_table}: {str(e)}\")\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ab530e6-439f-4dbc-a6c7-85596184e450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def append_or_create_table(param_df, param_target_table):\n",
    "    try:\n",
    "        spark.read.table(param_target_table)\n",
    "        param_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(param_target_table)\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        param_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(param_target_table)\n",
    "        return True\n",
    "    except Exception as v_error:\n",
    "        print(f\"Error processing {param_target_table}: {str(v_error)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97116f6f-9571-4b67-bb8f-8f1d7aa4936f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import DataFrame\n",
    "# from pyspark.sql.functions import expr, current_timestamp\n",
    "\n",
    "# def add_audit_fields(df, clientid, facilityid):\n",
    "#     result = (\n",
    "#         df.withColumn(\"client_id\", clientid)\n",
    "#         .withColumn(\"facility_id\", facilityid)\n",
    "#         .withColumn(\"created_by_user\", expr(\"current_user()\"))\n",
    "#         .withColumn(\"process_timestamp\", current_timestamp())\n",
    "#         .withColumn(\"modified_by_user\",  expr(\"current_user()\"))\n",
    "#         .withColumn(\"datetime_last_modified\", current_timestamp())\n",
    "#     )\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65be1cde-fd64-415a-a596-92b44349b715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import expr, current_timestamp\n",
    "\n",
    "def add_audit_fields(param_df, param_client_id, param_facility_id):\n",
    "    v_result_df = (\n",
    "        param_df.withColumn(\"client_id\", param_client_id)\n",
    "        .withColumn(\"facility_id\", param_facility_id)\n",
    "        .withColumn(\"created_by_user\", expr(\"current_user()\"))\n",
    "        .withColumn(\"process_timestamp\", current_timestamp())\n",
    "        .withColumn(\"modified_by_user\", expr(\"current_user()\"))\n",
    "        .withColumn(\"datetime_last_modified\", current_timestamp())\n",
    "    )\n",
    "    return v_result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf7db6e5-103e-4724-9fe4-8b878c3a537c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "\n",
    "# from pyspark.sql.functions import pandas_udf\n",
    "# from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# @pandas_udf(\"array<string>\")\n",
    "# def parse_edi_udf(content_series: pd.Series, filename_series: pd.Series) -> pd.Series:\n",
    "#     results = []\n",
    "#     for content, filename in zip(content_series, filename_series):\n",
    "#         try:\n",
    "#             edi_obj = EDI(content, strict_transactions=False)\n",
    "#             flattened = hm.flatten2(edi_obj, filename=filename)\n",
    "#             json_rows = [json.dumps(hm.flatten_to_json3(row)) for row in flattened]\n",
    "#             results.append(json_rows)\n",
    "#         except Exception as e1:\n",
    "#             try:\n",
    "#                 edi_obj = EDI(content)\n",
    "#                 flattened = hm.flatten2(edi_obj, filename=filename)\n",
    "#                 json_rows = [json.dumps(hm.flatten_to_json3(row)) for row in flattened]\n",
    "#                 results.append(json_rows)\n",
    "#             except Exception as e2:\n",
    "#                 error_info = {\n",
    "#                     \"filename\": filename,\n",
    "#                     \"error_first_attempt\": str(e1),\n",
    "#                     \"error_second_attempt\": str(e2),\n",
    "#                     \"quarantine\": True\n",
    "#                 }\n",
    "#                 results.append([json.dumps(error_info)])\n",
    "#     return pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a714dab-9278-49ba-b21a-418e300d4186",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "@pandas_udf(\"array<string>\")\n",
    "def parse_edi_udf(param_content_series: pd.Series, param_filename_series: pd.Series) -> pd.Series:\n",
    "    v_results_list = []\n",
    "\n",
    "    for v_content, v_filename in zip(param_content_series, param_filename_series):\n",
    "        try:\n",
    "            v_edi_obj = EDI(v_content, strict_transactions=False)\n",
    "            v_flattened_list = hm.flatten2(v_edi_obj, filename=v_filename)\n",
    "            v_json_rows = [json.dumps(hm.flatten_to_json3(v_row)) for v_row in v_flattened_list]\n",
    "            v_results_list.append(v_json_rows)\n",
    "        except Exception as v_error_first:\n",
    "            try:\n",
    "                v_edi_obj = EDI(v_content)\n",
    "                v_flattened_list = hm.flatten2(v_edi_obj, filename=v_filename)\n",
    "                v_json_rows = [json.dumps(hm.flatten_to_json3(v_row)) for v_row in v_flattened_list]\n",
    "                v_results_list.append(v_json_rows)\n",
    "            except Exception as v_error_second:\n",
    "                v_error_info = {\n",
    "                    \"filename\": v_filename,\n",
    "                    \"error_first_attempt\": str(v_error_first),\n",
    "                    \"error_second_attempt\": str(v_error_second),\n",
    "                    \"quarantine\": True\n",
    "                }\n",
    "                v_results_list.append([json.dumps(v_error_info)])\n",
    "\n",
    "    return pd.Series(v_results_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "850bb5ee-e78e-4f1e-89e1-890a3299a56f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "@pandas_udf(\"array<string>\")\n",
    "def parse_edi_udf0(param_content_series: pd.Series, param_filename_series: pd.Series) -> pd.Series:\n",
    "    v_results_list = []\n",
    "\n",
    "    for v_content, v_filename in zip(param_content_series, param_filename_series):\n",
    "        try:\n",
    "            v_edi_obj = EDI(v_content, strict_transactions=False)\n",
    "            v_flattened_list = hm.flatten(v_edi_obj, filename=v_filename)\n",
    "            v_json_rows = [json.dumps(hm.flatten_to_json(v_row)) for v_row in v_flattened_list]\n",
    "            v_results_list.append(v_json_rows)\n",
    "        except Exception as v_error_first:\n",
    "            try:\n",
    "                v_edi_obj = EDI(v_content)\n",
    "                v_flattened_list = hm.flatten(v_edi_obj, filename=v_filename)\n",
    "                v_json_rows = [json.dumps(hm.flatten_to_json(v_row)) for v_row in v_flattened_list]\n",
    "                v_results_list.append(v_json_rows)\n",
    "            except Exception as v_error_second:\n",
    "                v_error_info = {\n",
    "                    \"filename\": v_filename,\n",
    "                    \"error_first_attempt\": str(v_error_first),\n",
    "                    \"error_second_attempt\": str(v_error_second),\n",
    "                    \"quarantine\": True\n",
    "                }\n",
    "                v_results_list.append([json.dumps(v_error_info)])\n",
    "\n",
    "    return pd.Series(v_results_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f2df7f1-7585-4b9c-bf91-5c10d5d77ed7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import *\n",
    "# import json\n",
    "\n",
    "\n",
    "# def parse_and_flatten_edi(content, filename):\n",
    "#     try:\n",
    "#         try:\n",
    "#             edi = EDI(content)\n",
    "#         except Exception:\n",
    "#             edi = EDI(content, strict_transactions=False)\n",
    "\n",
    "#         if not edi or (isinstance(edi, list) and len(edi) == 0):\n",
    "#             raise ValueError(\"Parsed EDI content is empty.\")\n",
    "\n",
    "#         flattened = hm.flatten(edi, filename=filename)\n",
    "\n",
    "#         if not flattened or (isinstance(flattened, list) and len(flattened) == 0):\n",
    "#             raise ValueError(\"Flattened output is empty.\")\n",
    "\n",
    "#         return [json.dumps(hm.flatten_to_json(x)) for x in flattened]\n",
    "\n",
    "#     except Exception as e:\n",
    "#         return [\n",
    "#             json.dumps({\"filename\": filename, \"error\": str(e), \"quarantined\": True})\n",
    "#         ]\n",
    "\n",
    "\n",
    "# parse_and_flatten_udf = udf(parse_and_flatten_edi, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "# from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "# def process_table(df, target_table):\n",
    "#     \"\"\"Append to an existing table or create a new one.\"\"\"\n",
    "#     try:\n",
    "#         spark.read.table(target_table)\n",
    "\n",
    "#         df.write.format(\"delta\").mode(\"append\").option(\n",
    "#             \"mergeSchema\", \"true\"\n",
    "#         ).saveAsTable(target_table)\n",
    "#         return True\n",
    "#     except AnalysisException:\n",
    "#         df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {target_table}: {str(e)}\")\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fad915eb-3bb3-4f32-a462-cbac4500a664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import json\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def parse_and_flatten_edi(param_content, param_filename):\n",
    "    try:\n",
    "        try:\n",
    "            v_edi_obj = EDI(param_content)\n",
    "        except Exception:\n",
    "            v_edi_obj = EDI(param_content, strict_transactions=False)\n",
    "\n",
    "        if not v_edi_obj or (isinstance(v_edi_obj, list) and len(v_edi_obj) == 0):\n",
    "            raise ValueError(\"Parsed EDI content is empty.\")\n",
    "\n",
    "        v_flattened_list = hm.flatten(v_edi_obj, filename=param_filename)\n",
    "\n",
    "        if not v_flattened_list or (isinstance(v_flattened_list, list) and len(v_flattened_list) == 0):\n",
    "            raise ValueError(\"Flattened output is empty.\")\n",
    "\n",
    "        return [json.dumps(hm.flatten_to_json(v_row)) for v_row in v_flattened_list]\n",
    "\n",
    "    except Exception as v_error:\n",
    "        return [\n",
    "            json.dumps({\n",
    "                \"filename\": param_filename,\n",
    "                \"error\": str(v_error),\n",
    "                \"quarantined\": True\n",
    "            })\n",
    "        ]\n",
    "\n",
    "parse_and_flatten_udf = udf(parse_and_flatten_edi, ArrayType(StringType()))\n",
    "\n",
    "def process_table(param_df, param_target_table):\n",
    "    \"\"\"Append to an existing table or create a new one.\"\"\"\n",
    "    try:\n",
    "        spark.read.table(param_target_table)\n",
    "\n",
    "        param_df.write.format(\"delta\").mode(\"append\").option(\n",
    "            \"mergeSchema\", \"true\"\n",
    "        ).saveAsTable(param_target_table)\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        param_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(param_target_table)\n",
    "        return True\n",
    "    except Exception as v_error:\n",
    "        print(f\"Error processing {param_target_table}: {str(v_error)}\")\n",
    "        return False\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ParserCommonFunctions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
