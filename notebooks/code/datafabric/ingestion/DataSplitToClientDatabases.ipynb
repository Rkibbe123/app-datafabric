{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31963713-9dad-4ae6-a400-059c8a4fe3c4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Widget Setup"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"p_cdc_instance_name\", \"\", \"CDC Instance Name\")\n",
    "dbutils.widgets.text(\"p_timestamp\", \"\", \"PipelineStartTime\")\n",
    "dbutils.widgets.text(\"p_product_id\", \"\", \"Product ID\")\n",
    "dbutils.widgets.text(\"p_environment_name\", \"\", \"Environment Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412e719e-0e49-4f40-8263-f4ee989f9a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_widget_values():\n",
    "    v_widget_values_dict = {\n",
    "        \"filter_product_id\"       : dbutils.widgets.get(\"p_product_id\").strip(),\n",
    "        \"filter_timestamp\"        : dbutils.widgets.get(\"p_timestamp\").strip(),\n",
    "        \"filter_cdc_instance_name\": dbutils.widgets.get(\"p_cdc_instance_name\").strip(),\n",
    "        \"filter_environment_name\" : dbutils.widgets.get(\"p_environment_name\").strip()\n",
    "    }\n",
    "\n",
    "    v_required_widgets_list = [\n",
    "        \"filter_product_id\",\"filter_timestamp\", \"filter_cdc_instance_name\",\"filter_environment_name\"\n",
    "    ]\n",
    "\n",
    "    for widget in v_required_widgets_list:\n",
    "        if not v_widget_values_dict[widget]:\n",
    "            raise ValueError(f\"The widget '{widget}' must be populated before continuing.\")\n",
    "\n",
    "    return v_widget_values_dict\n",
    "\n",
    "try:\n",
    "    v_widget_values          = get_widget_values()\n",
    "    v_product_id          = v_widget_values[\"filter_product_id\"]\n",
    "    v_timestamp           = v_widget_values[\"filter_timestamp\"]\n",
    "    v_cdc_instance_name   = v_widget_values[\"filter_cdc_instance_name\"]\n",
    "    v_environment         = v_widget_values[\"filter_environment_name\"]\n",
    "\n",
    "    print(\"All required widgets are populated. Proceeding with the script.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f44e598c-040e-49a2-a069-fe4ad740c411",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import packages"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    max,\n",
    "    expr,\n",
    "    current_timestamp,\n",
    "    regexp_replace,\n",
    "    upper,\n",
    "    when,\n",
    "    lit,\n",
    "    lower,\n",
    "    concat\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "from datetime import datetime\n",
    "import threading\n",
    "\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from functools import reduce\n",
    "import traceback\n",
    "from typing import Dict\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07da2393-025b-4081-836d-05c5fabc5235",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DataFabricCommonFunctions"
    }
   },
   "outputs": [],
   "source": [
    "%run \"../common/DataFabricCommonFunctions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a810f4e-bba5-48b9-9fee-e63086ab774c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Getting environment variables"
    }
   },
   "outputs": [],
   "source": [
    "v_locations_dict = get_locations_by_env(v_environment)\n",
    "\n",
    "v_file_location        = v_locations_dict['source']\n",
    "v_archive_location     = v_locations_dict['archive']\n",
    "v_key_vault_location   = v_locations_dict['keyvault']\n",
    "v_unity_catalog        = v_locations_dict['unity_catalog']\n",
    "\n",
    "v_timestamp = datetime.strptime(v_timestamp, \"%Y-%m-%d %H:%M:%S.%f\").strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a85624f4-b416-4e05-b1c8-1dd9f22e0125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Object Filtering Strategy: SQL Server vs Unity Catalog\n",
    "\n",
    "After retrieving the list of source objects (tables/views) from SQL Server, we apply a filtering step to ensure that only the relevant and existing objects in **Unity Catalog** are processed.\n",
    "\n",
    "#### Why This Matters\n",
    "- Prevents unnecessary computation on tables that don't exist in Unity Catalog.\n",
    "- Reduces risk of errors from referencing non-existent schemas or tables.\n",
    "- Improves performance by limiting the scope of the loop to valid targets.\n",
    "\n",
    "#### Process Overview\n",
    "1. **Extract Object Metadata** from SQL Server (e.g., `Database`, `TableName`).\n",
    "2. **List Available Objects** in Unity Catalog using Spark’s catalog introspection.\n",
    "3. **Match & Filter**: Perform an inner join or intersection to retain only those objects that exist in both sources.\n",
    "4. **Run Loop**: Iterate only on the matched objects for extraction, transformation, and loading (ETL).\n",
    "\n",
    "This ensures your pipeline remains resilient and efficient while aligning strictly with Unity Catalog's metadata registry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df9d95a8-df47-4a8d-b6bd-2cf99e9e859c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main query for data split"
    }
   },
   "outputs": [],
   "source": [
    "v_sql_server_df = (\n",
    "    get_table_process_list(v_environment)\n",
    "    .filter(col('CDCDestinationSchema') == f'{v_cdc_instance_name}')\n",
    "    .withColumnRenamed('SourceFacilityId', 'SiteId')\n",
    "    .withColumnRenamed('SourceServerName1', 'OriginalInstance')\n",
    "    .withColumnRenamed('SourceDatabaseName1', 'DatabaseName')\n",
    "    .withColumnRenamed('CDCKeyColumns', 'PK')\n",
    "    .withColumn('UC_SchemaName', lower(col('CDCDestinationSchema')))\n",
    "    .withColumn('TableName', lower(col('SourceTable')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc51e4ba-13d0-4b1e-989f-de8bdbba3905",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Objects available in catalog"
    }
   },
   "outputs": [],
   "source": [
    "v_uc_schema_df = sql(f\"SHOW SCHEMAS IN {v_unity_catalog}\")\n",
    "v_uc_schema_list = [row['databaseName'] for row in v_uc_schema_df.collect()]\n",
    "\n",
    "v_all_schema_tables_list = []\n",
    "\n",
    "for v_uc_schema_name in v_uc_schema_list:\n",
    "    v_schema_tables_df = sql(f\"SHOW TABLES IN {v_unity_catalog}.{v_uc_schema_name}\").select('Database', 'TableName')\n",
    "    v_all_schema_tables_list.append(v_schema_tables_df)\n",
    "\n",
    "v_uc_tables_df = reduce(lambda x, y: x.unionByName(y), v_all_schema_tables_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e595fb-f417-4a55-b84d-699cd10c75dd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Getting intersection of UC and SQL server objects"
    }
   },
   "outputs": [],
   "source": [
    "v_sql_server_with_priority_df = v_sql_server_df.withColumn(\n",
    "    \"db_priority\",\n",
    "    when(v_sql_server_df[\"DatabaseName\"] == \"Archive\", 1)\n",
    "    .when(v_sql_server_df[\"DatabaseName\"] == \"Primary\", 2)\n",
    "    .otherwise(3),\n",
    ")\n",
    "\n",
    "v_execution_target_obj_df = (\n",
    "    v_sql_server_with_priority_df.join(\n",
    "        v_uc_tables_df.withColumnRenamed(\"Database\", \"UC_SchemaName\"),\n",
    "        [\"UC_SchemaName\", \"TableName\"],\n",
    "        \"inner\",\n",
    "    )\n",
    "    .distinct()\n",
    "    .orderBy(\"db_priority\", \"UC_SchemaName\", \"TableName\", \"InternalClientId\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae7f514-0d2e-4e97-b37c-c95b2894d188",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregate isHistorical by Client + Table"
    }
   },
   "outputs": [],
   "source": [
    "v_agg_is_historical_df = v_execution_target_obj_df.groupBy(\n",
    "    \"UC_SchemaName\", \"DatabaseName\", \"InternalClientId\", \"TableName\", \"DataSourceShortName\"\n",
    ").agg(\n",
    "    max(\"IsHistorical\").alias(\"IsHistorical\")\n",
    ")\n",
    "\n",
    "v_agg_is_historical_true_df = v_agg_is_historical_df.filter(\n",
    "    col(\"IsHistorical\") == True\n",
    ").withColumn(\n",
    "    \"table_path\",\n",
    "    concat(\n",
    "        lit(v_unity_catalog),\n",
    "        lit(\".db_\"),\n",
    "        col(\"InternalClientId\"),\n",
    "        lit(\".\"),\n",
    "        col(\"DataSourceShortName\"),\n",
    "        lit(\"_ods_\"),\n",
    "        col(\"TableName\")\n",
    "    )\n",
    ")\n",
    "\n",
    "v_overwrite_table_list = (\n",
    "    v_agg_is_historical_true_df\n",
    "    .select(\"table_path\")\n",
    "    .distinct()\n",
    "    .toPandas()[\"table_path\"]\n",
    "    .tolist()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f3362bf-3750-4e4c-8ff8-954d748e6da4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup client db"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "spark.sql(f\"USE CATALOG {v_unity_catalog}\")\n",
    "\n",
    "for row in v_execution_target_obj_df.select(\"InternalClientId\").distinct().collect():\n",
    "    v_client_id = row[\"InternalClientId\"]\n",
    "    v_target_db_name = f\"db_{v_client_id}\"\n",
    "    v_managed_location = f\"{v_file_location}{v_client_id}\"\n",
    "    try:\n",
    "        v_create_schema_stmt = (\n",
    "            f\"CREATE SCHEMA IF NOT EXISTS {v_unity_catalog}.{v_target_db_name} \"\n",
    "            f\"MANAGED LOCATION '{v_managed_location}'\"\n",
    "        )\n",
    "        print(v_create_schema_stmt)\n",
    "        spark.sql(v_create_schema_stmt)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create schema for {v_client_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3cd2400-a214-426f-940a-a06d0763ed66",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup common db"
    }
   },
   "outputs": [],
   "source": [
    "v_common_db_name = f\"db_common\"\n",
    "v_common_managed_location = f\"{v_file_location}common\"\n",
    "try:\n",
    "    v_create_schema_stmt = (\n",
    "        f\"CREATE SCHEMA IF NOT EXISTS {v_unity_catalog}.{v_common_db_name} \"\n",
    "        f\"MANAGED LOCATION '{v_common_managed_location}'\"\n",
    "    )\n",
    "    print(v_create_schema_stmt)\n",
    "    spark.sql(v_create_schema_stmt)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create schema for {v_client_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "494c0103-3350-409c-b7d2-c5dff5bd6be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Split ETL Functions – Reference\n",
    "---\n",
    "\n",
    "### Decision Guide for `handle_merge`\n",
    "- **Error if:**  \n",
    "  - *Historical* **and** no `partition_keys` **and** no `merge_condition` → raises exception.  \n",
    "  - *Non-historical* **and** no `merge_condition` → raises exception.\n",
    "- **Actions:**  \n",
    "  - If *historical* **and** `partition_keys` provided → first `DELETE` all rows with `InternalFacilityId = facility_id`, **then** proceed.  \n",
    "  - If `merge_condition` provided → `MERGE` (`whenMatchedUpdate` with all columns except `created_by_user` and `process_timestamp`; `whenNotMatchedInsertAll`).  \n",
    "  - Else (no `merge_condition`) → `append` (`mode=\"append\"`).\n",
    "\n",
    "> **Note:** The historical delete assumes the target table has `InternalFacilityId`. If it doesn’t, the `DELETE` will fail.\n",
    "\n",
    "---\n",
    "\n",
    "### Function Details\n",
    "\n",
    "#### `format_query(query_str, placeholder_dict, uc_schema_name, table_name)`\n",
    "**Purpose:** Return a fully formatted SQL string.  \n",
    "**Behavior:**\n",
    "- If `query_str` is provided, returns `query_str.format(**placeholder_dict)`.  \n",
    "  - ⚠️ Missing keys in `placeholder_dict` will raise a `KeyError`.  \n",
    "- Else returns `SELECT * FROM {uc_schema_name}.{table_name}`.\n",
    "\n",
    "---\n",
    "\n",
    "#### `add_audit_columns(df, client_id, facility_id, timestamp_str, schema_name)`\n",
    "**Purpose:** Stamp audit/lineage columns.  \n",
    "**Returns:** `None` if `df.count() == 0`, else a new DF with:\n",
    "- `created_by_user = current_user()`\n",
    "- `datetime_last_modified = timestamp_str`\n",
    "- `modified_by_user = current_user()`\n",
    "- `process_timestamp = timestamp_str`\n",
    "- `InternalClientId = client_id`\n",
    "- `InternalFacilityId = facility_id`\n",
    "- `UC_SchemaName = schema_name`\n",
    "\n",
    "---\n",
    "\n",
    "#### `get_merge_condition(pk_str, df, facility_id)`\n",
    "**Purpose:** Build a SQL `AND`-joined predicate for `DeltaTable.merge`.  \n",
    "**Behavior:**\n",
    "- From `pk_str=\"A,B,C\"`, produces `target.A = source.A AND ...`\n",
    "- If `InternalFacilityId` is a column **and** `facility_id` is not `None`, appends  \n",
    "  `target.InternalFacilityId = {facility_id} AND source.InternalFacilityId = {facility_id}`.\n",
    "- Returns `None` if no clauses built.\n",
    "\n",
    "---\n",
    "\n",
    "#### `update_watermark_value(v_product_id, original_instance_name, step_name_str, timestamp_str)`\n",
    "**Purpose:** Update status/watermark in your config DB via stored procedure.  \n",
    "**Notes:** Requires `execute_dbconfig_stored_procedure` and `v_environment`.\n",
    "\n",
    "---\n",
    "\n",
    "#### `handle_merge(df, table_path_str, merge_condition_str, partition_keys, facility_id, is_historical_flag)`\n",
    "**Purpose:** Centralized **merge/append** (and optional **historical delete**) into a Delta table.\n",
    "\n",
    "**Key logic:**\n",
    "- **Validation:**\n",
    "  - Historical & no partition keys & no merge condition → `raise`\n",
    "  - Non-historical & no merge condition → `raise`\n",
    "- **Historical deletion:** if historical **and** `partition_keys` provided →  \n",
    "  `DELETE FROM {table} WHERE InternalFacilityId = {facility_id}`\n",
    "- **Merge path:** if `merge_condition_str` present → `MERGE`:\n",
    "  - `whenMatchedUpdate(set=update_map)`  \n",
    "    - `update_map` includes **all** columns from `df` **except** `created_by_user` and `process_timestamp`  \n",
    "      (⚠️ exclusion is **case-sensitive**: `{\"created_by_user\",\"process_timestamp\"}`)\n",
    "  - `whenNotMatchedInsertAll()`\n",
    "- **Append path:** if no `merge_condition_str` → `df.write.format(\"delta\").mode(\"append\").saveAsTable(...)`\n",
    "\n",
    "---\n",
    "\n",
    "#### `overwrite_table_with_partition(df, table_path_str, v_partition_keys)`\n",
    "**Purpose:** Full overwrite write, optionally partitioned.  \n",
    "**Behavior:**  \n",
    "- If `v_partition_keys` non-empty → `partitionBy(*v_partition_keys)`  \n",
    "- Else → non-partitioned overwrite.\n",
    "\n",
    "---\n",
    "\n",
    "#### `get_partition_keys(df)`\n",
    "**Purpose:** Recommend partition keys present in DF.  \n",
    "**Current rule:** Returns `[\"InternalFacilityId\"]` if available; else `[]`.\n",
    "\n",
    "---\n",
    "\n",
    "#### `check_partitions(table_path_str, partition_keys)`\n",
    "**Purpose:** Determine if repartitioning is required.  \n",
    "**Behavior:**  \n",
    "- Returns `False` if `partition_keys ⊆ current_partition_columns` (no change needed)  \n",
    "- Returns `True` otherwise (table should be rewritten with desired partitions)\n",
    "\n",
    "---\n",
    "\n",
    "#### `change_partition_of(table_path_str, partition_keys, client_id, facility_id, v_timestamp, uc_schema_name)`\n",
    "**Purpose:** Rewrite table **with new partition spec** (and refresh audit columns).  \n",
    "**Behavior:**  \n",
    "- `spark.read.table(...)` → `add_audit_columns(...)` → `overwrite` with `partitionBy(*partition_keys)` and `option(\"overwriteSchema\",\"true\")`.  \n",
    "- ⚠️ Rewrites **entire** table; ensure you intend to repartition all data.\n",
    "\n",
    "---\n",
    "\n",
    "### Gotchas & Tips\n",
    "- `add_audit_columns` returns **`None`** for empty DataFrames; check before using.  \n",
    "- `update_map` exclusion in `handle_merge` is **case-sensitive**; ensure column names exactly match.  \n",
    "- `format_query` uses Python `str.format` – supply all placeholders or you’ll get a `KeyError`.  \n",
    "- `check_partitions` only checks subset/superset, not order; repartition if you need an exact match.  \n",
    "\n",
    "### Design Highlights\n",
    "✅ Modular and reusable functions  \n",
    "✅ Supports both merge and overwrite logic  \n",
    "✅ Thread-safe dictionary updates using lock  \n",
    "✅ Parallel execution for performance  \n",
    "✅ Audit tracking and watermarking for data lineage  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d924a7-8106-43b7-a3b2-c5644a803a3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def format_query(query_str, placeholder_dict, uc_schema_name, table_name):\n",
    "    if query_str:\n",
    "        return query_str.format(**placeholder_dict)\n",
    "    return f\"SELECT * FROM {uc_schema_name}.{table_name}\"\n",
    "\n",
    "\n",
    "def add_audit_columns(df, client_id, facility_id, timestamp_str, schema_name):\n",
    "    return (\n",
    "        df.withColumn(\"created_by_user\", expr(\"current_user()\"))\n",
    "        .withColumn(\"datetime_last_modified\", lit(timestamp_str))\n",
    "        .withColumn(\"modified_by_user\", expr(\"current_user()\"))\n",
    "        .withColumn(\"process_timestamp\", lit(timestamp_str))\n",
    "        .withColumn(\"InternalClientId\", lit(client_id))\n",
    "        .withColumn(\"InternalFacilityId\", lit(facility_id))\n",
    "        .withColumn(\"UC_SchemaName\", lit(schema_name))\n",
    "    )\n",
    "\n",
    "def get_merge_condition(pk_str, df, facility_id):\n",
    "    clauses = []\n",
    "\n",
    "    if pk_str:\n",
    "        pks = [p.strip() for p in pk_str.split(\",\") if p.strip()]\n",
    "        clauses.extend([f\"target.{p} = source.{p}\" for p in pks])\n",
    "    if \"InternalFacilityId\" in df.columns and facility_id is not None:\n",
    "        clauses.append(f\"target.InternalFacilityId = {facility_id} and source.InternalFacilityId = {facility_id}\")\n",
    "    \n",
    "    if not clauses:\n",
    "        return None\n",
    "    else:\n",
    "        return \" AND \".join(clauses)\n",
    "\n",
    "\n",
    "def update_watermark_value(v_product_id, original_instance_name, step_name_str, timestamp_str):\n",
    "    execute_dbconfig_stored_procedure(\n",
    "        f\"\"\"EXEC cfg.usp_UpdateSQLProcessStatusFacilityDetailWithWatermarkValue\n",
    "        @InternalProductId = '{v_product_id}',\n",
    "        @SourceInstanceServerName = '{original_instance_name}',\n",
    "        @StepName = '{step_name_str}',\n",
    "        @WatermarkValue = '{timestamp_str}'\"\"\",\n",
    "        v_environment\n",
    "    )\n",
    "\n",
    "def handle_merge(df, table_path_str, merge_condition_str, partition_keys, facility_id, is_historical_flag, schema_name, pk_str):\n",
    "\n",
    "    def _is_blank(s):\n",
    "        return s is None or str(s).strip().lower() in {\"\", \"none\", \"null\"}\n",
    "\n",
    "    excluded = {\"created_by_user\", \"process_timestamp\"}\n",
    "    update_map = {c: f\"source.{c}\" for c in df.columns if c not in excluded}\n",
    "    delta_table = DeltaTable.forName(spark, table_path_str)\n",
    "\n",
    "    if \"InternalFacilityId\" in df.columns and facility_id is not None:\n",
    "        df = df.filter(col(\"InternalFacilityId\") == facility_id)\n",
    "\n",
    "    do_delete = bool(\n",
    "                    (is_historical_flag) or\n",
    "                    (not is_historical_flag and not pk_str)\n",
    "                    )\n",
    "\n",
    "    do_merge  = (not _is_blank(merge_condition_str) and not _is_blank(pk_str))\n",
    "\n",
    "    if do_delete:\n",
    "        delete_query = f\"\"\"DELETE FROM {table_path_str}\"\"\"\n",
    "        if schema_name and facility_id is not None:\n",
    "            delete_query += f\"\"\" WHERE InternalFacilityId = {facility_id} and UC_SchemaName = '{schema_name}'\"\"\"\n",
    "        spark.sql(delete_query)\n",
    "\n",
    "    if do_merge:\n",
    "        (\n",
    "            delta_table.alias(\"target\")\n",
    "            .merge(df.alias(\"source\"), merge_condition_str)\n",
    "            .whenMatchedUpdate(set=update_map)\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "    else:\n",
    "        df.write.format(\"delta\").mode(\"append\").saveAsTable(table_path_str)\n",
    "        \n",
    "\n",
    "\n",
    "def overwrite_table_with_partition(df, table_path_str, v_partition_keys):\n",
    "\n",
    "    if v_partition_keys:\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").partitionBy(*v_partition_keys).saveAsTable(table_path_str)\n",
    "    else:\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_path_str)\n",
    "\n",
    "\n",
    "def get_partition_keys(df):\n",
    "    \n",
    "    v_candidate_partition_keys = [\"InternalFacilityId\"]\n",
    "\n",
    "    v_partition_keys_in_df = [k for k in v_candidate_partition_keys if k in df.columns]\n",
    "\n",
    "    if v_partition_keys_in_df:\n",
    "        return v_partition_keys_in_df\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def check_partitions(table_path_str, partition_keys):\n",
    "    detail = spark.sql(f\"DESCRIBE DETAIL {table_path_str}\").first()\n",
    "    current_partitions = list(detail[\"partitionColumns\"] or [])\n",
    "\n",
    "    if set(partition_keys).issubset(set(current_partitions)):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def change_partition_of(table_path_str, partition_keys, client_id, facility_id, v_timestamp, uc_schema_name):\n",
    "    print(f\"Updating partition of {table_path_str}\")\n",
    "    df = spark.read.table(table_path_str)\n",
    "    df = add_audit_columns(df, client_id, facility_id, v_timestamp, uc_schema_name)\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(*partition_keys).saveAsTable(table_path_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718aa560-992f-45fe-a2b4-7ab4a415b591",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Modularized processes"
    }
   },
   "outputs": [],
   "source": [
    "def process_row(\n",
    "    row_dict,\n",
    "    v_unity_catalog,\n",
    "    v_product_id,\n",
    "    v_timestamp,\n",
    "    overwrite_table_list\n",
    "):\n",
    "    log_messages = []\n",
    "    try:\n",
    "        original_instance_name = row_dict[\"OriginalInstance\"]\n",
    "        uc_schema_name = row_dict[\"UC_SchemaName\"]\n",
    "        database_name = row_dict[\"DatabaseName\"]\n",
    "        table_name = row_dict[\"TableName\"]\n",
    "        pk_str = row_dict[\"PK\"]\n",
    "        site_id = row_dict[\"SiteId\"]\n",
    "        inc_query_str = row_dict[\"IncrementalExtractQuery\"]\n",
    "        hist_query_str = row_dict[\"HistoricalExtractQuery\"]\n",
    "        watermark_value = row_dict[\"WatermarkValue\"]\n",
    "        is_historical_flag = row_dict[\"IsHistorical\"]\n",
    "        datasource_name = row_dict[\"DataSourceShortName\"]\n",
    "        step_name_str = f\"{database_name}.{table_name}\"\n",
    "        v_internal_facility_id = row_dict[\"InternalFacilityId\"]\n",
    "        v_internal_client_id = row_dict[\"InternalClientId\"]\n",
    "        v_datasource_id = row_dict[\"DataSourceId\"]\n",
    "\n",
    "        query_str = hist_query_str if is_historical_flag else inc_query_str\n",
    "        placeholder_dict = {\n",
    "            \"UC_SchemaName\": uc_schema_name,\n",
    "            \"TableName\": table_name,\n",
    "            \"SiteId\": site_id,\n",
    "            \"WatermarkValue\": f\"'{watermark_value}'\",\n",
    "        }\n",
    "        formatted_query_str = format_query(query_str, placeholder_dict, uc_schema_name, table_name)\n",
    "        log_messages.append(f\"query = {formatted_query_str}\")\n",
    "        df = spark.sql(formatted_query_str)\n",
    "        db_name = \"db_common\" if v_internal_client_id == 0 else f\"db_{v_internal_client_id}\"\n",
    "        table_path_str = f\"{v_unity_catalog}.{db_name}.{datasource_name}_ods_{table_name}\"   \n",
    "        df = add_audit_columns(df, v_internal_client_id, v_internal_facility_id, v_timestamp, uc_schema_name)\n",
    "        row_count = df.count()\n",
    "        log_messages.append(f\"DF record count = {row_count}\")\n",
    "        \n",
    "        merge_condition_str = get_merge_condition(pk_str, df, v_internal_facility_id)\n",
    "        log_messages.append(f\"Merge_condition = {merge_condition_str}\")\n",
    "        v_partition_keys = get_partition_keys(df)\n",
    "\n",
    "        table_exists = spark.catalog.tableExists(table_path_str)\n",
    "\n",
    "        if v_partition_keys and table_exists:\n",
    "            v_needs_partition = check_partitions(table_path_str, v_partition_keys)\n",
    "            if v_needs_partition:\n",
    "                change_partition_of(table_path_str, v_partition_keys, v_internal_client_id, v_internal_facility_id, v_timestamp, uc_schema_name)\n",
    "\n",
    "        if not table_exists:\n",
    "            v_table_state = 'created'\n",
    "            try:\n",
    "                overwrite_table_with_partition(df, table_path_str, v_partition_keys)\n",
    "            except:\n",
    "                handle_merge(df, table_path_str, merge_condition_str, v_partition_keys, v_internal_facility_id, is_historical_flag, uc_schema_name, pk_str)\n",
    "                v_table_state = 'updated'\n",
    "            log_messages.append(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ==> Table {table_path_str} was {v_table_state} successfully.\")\n",
    "            update_process_status_facility_detail('S',\n",
    "                                            v_product_id,\n",
    "                                            v_internal_client_id,\n",
    "                                            v_internal_facility_id,\n",
    "                                            step_name_str,\n",
    "                                            v_datasource_id,\n",
    "                                            v_environment)\n",
    "            return log_messages\n",
    "        else:\n",
    "            handle_merge(df, table_path_str, merge_condition_str, v_partition_keys, v_internal_facility_id, is_historical_flag, uc_schema_name, pk_str)\n",
    "            log_messages.append(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ==> Table {table_path_str} updated successfully.\")\n",
    "            update_process_status_facility_detail('S',\n",
    "                                            v_product_id,\n",
    "                                            v_internal_client_id,\n",
    "                                            v_internal_facility_id,\n",
    "                                            step_name_str,\n",
    "                                            v_datasource_id,\n",
    "                                            v_environment)\n",
    "            return log_messages\n",
    "\n",
    "    except Exception as e:\n",
    "        log_messages.append(f\"Exception occurred: {str(e)}\")\n",
    "        update_process_status_facility_detail('F',\n",
    "                                                v_product_id,\n",
    "                                                v_internal_client_id,\n",
    "                                                v_internal_facility_id,\n",
    "                                                step_name_str,\n",
    "                                                v_datasource_id,\n",
    "                                                v_environment)\n",
    "        err = ValueError(f\"Error occurred: {str(e)}\")\n",
    "        err.logs = log_messages\n",
    "        raise err "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df4554e9-2dba-4dc0-9f11-dae906a816db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_row_with_retries(row_dict, v_unity_catalog, v_product_id, v_timestamp,\n",
    "                             overwrite_table_list, max_retries=3):\n",
    "    attempt = 1\n",
    "    outer_logs = []\n",
    "\n",
    "\n",
    "    outer_logs.append(\n",
    "        f\"==============> Processing row: {row_dict['UC_SchemaName']}.{row_dict['TableName']}, \"\n",
    "        f\"ClientId={row_dict['InternalClientId']}, SiteId={row_dict['SiteId']}\"\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        inner_logs = []\n",
    "        try:\n",
    "\n",
    "            inner_logs = process_row(row_dict, v_unity_catalog, v_product_id, v_timestamp, overwrite_table_list) or []\n",
    "\n",
    "            # ✅ Success: full logs on first attempt, short summary on retries\n",
    "            if attempt == 1:\n",
    "                outer_logs.extend(inner_logs)\n",
    "            else:\n",
    "                candidates = [l for l in inner_logs if (\"updated successfully\" in l or \"was \" in l or \"ERROR:\" in l)]\n",
    "                outer_logs.append(candidates[-1] if candidates else inner_logs[-1])\n",
    "\n",
    "            outer_logs.append(f\"[Attempt {attempt}/{max_retries}] ✅ {row_dict['UC_SchemaName']}.{row_dict['TableName']}\")\n",
    "            print(\"\\n\".join(outer_logs))\n",
    "            return {\"status\": \"success\", \"attempts\": attempt, \"logs\": outer_logs}\n",
    "\n",
    "        except Exception as e:\n",
    "            # ✅ Failure: harvest + keep only a short result line\n",
    "            inner_logs = getattr(e, \"logs\", inner_logs) or []\n",
    "            candidates = [l for l in inner_logs if (\"ERROR:\" in l or \"updated successfully\" in l or \"was \" in l)]\n",
    "            summary_line = candidates[-1] if candidates else (inner_logs[-1] if inner_logs else None)\n",
    "            if summary_line:\n",
    "                outer_logs.append(summary_line)\n",
    "\n",
    "            if attempt >= max_retries:\n",
    "                outer_logs.append(f\"[Attempt {attempt}/{max_retries}] ❌ {row_dict['UC_SchemaName']}.{row_dict['TableName']} failed\")\n",
    "                print(\"\\n\".join(outer_logs))\n",
    "                return {\"status\": \"failed\", \"error\": str(e), \"attempts\": attempt, \"logs\": outer_logs}\n",
    "\n",
    "            outer_logs.append(f\"[Attempt {attempt}/{max_retries}] ⚠️ {row_dict['UC_SchemaName']}.{row_dict['TableName']} error: {e} — retrying in 5s\")\n",
    "            time.sleep(5)\n",
    "            attempt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a554b8a8-8a27-488c-9631-bd6b7be23f1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor() as v_executor:\n",
    "    v_futures_list = [\n",
    "        v_executor.submit(\n",
    "            process_row_with_retries,\n",
    "            row,\n",
    "            v_unity_catalog,\n",
    "            v_product_id,\n",
    "            v_timestamp,\n",
    "            v_overwrite_table_list,\n",
    "            max_retries=3\n",
    "        )\n",
    "        for row in v_execution_target_obj_df.distinct().collect()\n",
    "    ]\n",
    "\n",
    "    concurrent.futures.wait(v_futures_list)\n",
    "\n",
    "    v_results = []\n",
    "    for future in v_futures_list:\n",
    "        try:\n",
    "            v_results.append(future.result())  \n",
    "        except Exception as e:\n",
    "            print(f\"[Thread] unexpected error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataSplitToClientDatabases",
   "widgets": {
    "p_cdc_instance_name": {
     "currentValue": "",
     "nuid": "f8374d58-2051-47ae-8d9f-bd53ff458f01",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "CDC Instance Name",
      "name": "p_cdc_instance_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "CDC Instance Name",
      "name": "p_cdc_instance_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_environment_name": {
     "currentValue": "",
     "nuid": "c5a09a00-d49b-4cad-a125-4ca36dd772a5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Environment Name",
      "name": "p_environment_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Environment Name",
      "name": "p_environment_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_product_id": {
     "currentValue": "",
     "nuid": "7be01e8a-c18f-4290-8395-da6f6c8cfa98",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Product ID",
      "name": "p_product_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Product ID",
      "name": "p_product_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_timestamp": {
     "currentValue": "",
     "nuid": "06f70608-86d5-4294-88b6-f1207d43670a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "PipelineStartTime",
      "name": "p_timestamp",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "PipelineStartTime",
      "name": "p_timestamp",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
